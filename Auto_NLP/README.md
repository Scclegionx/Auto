# PhoBERT_SAM - Há»‡ Thá»‘ng NLP Cho NgÆ°á»i Cao Tuá»•i

Há»‡ thá»‘ng Intent Recognition vÃ  Entity Extraction sá»­ dá»¥ng PhoBERT cho ngÆ°á»i cao tuá»•i, há»— trá»£ nháº­n diá»‡n Ã½ Ä‘á»‹nh vÃ  trÃ­ch xuáº¥t thÃ´ng tin tá»« cÃ¢u nÃ³i tiáº¿ng Viá»‡t.

## ğŸš€ TÃ­nh NÄƒng ChÃ­nh

- **Intent Recognition**: Nháº­n diá»‡n Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng (gá»i Ä‘iá»‡n, gá»­i tin nháº¯n, Ä‘áº·t bÃ¡o thá»©c, v.v.)
- **Entity Extraction**: TrÃ­ch xuáº¥t thÃ´ng tin quan trá»ng (ngÆ°á»i nháº­n, thá»i gian, tin nháº¯n, Ä‘á»‹a Ä‘iá»ƒm)
- **Command Mapping**: Chuyá»ƒn Ä‘á»•i intent thÃ nh command thá»±c thi
- **Value Generation**: Táº¡o mÃ´ táº£ hÃ nh Ä‘á»™ng cá»¥ thá»ƒ
- **Web Interface**: Giao diá»‡n web Ä‘áº¹p máº¯t Ä‘á»ƒ test API
- **RESTful API**: API hoÃ n chá»‰nh vá»›i FastAPI

## ğŸ“‹ YÃªu Cáº§u Há»‡ Thá»‘ng

- Python 3.8+
- RAM: 8GB+ (khuyáº¿n nghá»‹ 16GB)
- Disk: 5GB+ trá»‘ng
- OS: Windows/Linux/macOS

## ğŸ› ï¸ CÃ i Äáº·t

### 1. Clone Repository
```bash
git clone <repository-url>
cd Auto_NLP
```

### 2. CÃ i Äáº·t Dependencies
```bash
pip install -r requirements.txt
```

### 3. Kiá»ƒm Tra CÃ i Äáº·t
```bash
python -c "import torch; print('PyTorch:', torch.__version__)"
python -c "import transformers; print('Transformers:', transformers.__version__)"
```

## ğŸ¯ Sá»­ Dá»¥ng Nhanh

### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng API Server
```bash
python api_server.py
```
Server sáº½ cháº¡y táº¡i: http://localhost:8000

### BÆ°á»›c 2: Má»Ÿ Giao Diá»‡n Web
- Má»Ÿ file `web_interface.html` trong trÃ¬nh duyá»‡t
- Hoáº·c truy cáº­p: http://localhost:8000/docs (API documentation)

### BÆ°á»›c 3: Test Há»‡ Thá»‘ng
Nháº­p cÃ¡c cÃ¢u máº«u:
- "nháº¯c tÃ´i lÃºc 5 giá» chiá»u"
- "alo cho bá»‘"
- "gá»­i tin nháº¯n cho máº¹ ráº±ng tá»‘i con sáº½ vá» Äƒn cÆ¡m"
- "Ä‘áº·t bÃ¡o thá»©c lÃºc 7 giá» sÃ¡ng"

## ğŸ”§ Training Model (Náº¿u Cáº§n)

### 1. Chuáº©n Bá»‹ Dataset
Há»‡ thá»‘ng Ä‘Ã£ cÃ³ sáºµn dataset:
- `elderly_command_dataset_reduced.json`: 1,951 samples
- `nlp_command_dataset.json`: 3,062 samples

### 2. Data Augmentation (TÃ¹y chá»n)
```bash
python data_augmentation.py
```

### 3. Training Model
```bash
python simple_train.py
```

**LÆ°u Ã½**: Training cÃ³ thá»ƒ máº¥t 30-60 phÃºt tÃ¹y cáº¥u hÃ¬nh mÃ¡y.

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
Auto_NLP/
â”œâ”€â”€ api_server.py              # FastAPI server chÃ­nh
â”œâ”€â”€ web_interface.html         # Giao diá»‡n web
â”œâ”€â”€ simple_train.py            # Script training Ä‘Æ¡n giáº£n
â”œâ”€â”€ config.py                  # Cáº¥u hÃ¬nh há»‡ thá»‘ng
â”œâ”€â”€ data_augmentation.py       # TÄƒng cÆ°á»ng dá»¯ liá»‡u
â”œâ”€â”€ utils.py                   # Tiá»‡n Ã­ch
â”œâ”€â”€ main.py                    # Training pipeline Ä‘áº§y Ä‘á»§
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ TRAINING_GUIDE.md          # HÆ°á»›ng dáº«n training chi tiáº¿t
â”œâ”€â”€ models/                    # ThÆ° má»¥c chá»©a model
â”‚   â”œâ”€â”€ best_simple_intent_model.pth  # Model Ä‘Ã£ train
â”‚   â”œâ”€â”€ intent_model.py        # Model architecture
â”‚   â”œâ”€â”€ entity_model.py        # Entity extraction model
â”‚   â”œâ”€â”€ command_model.py       # Command processing model
â”‚   â””â”€â”€ unified_model.py       # Unified model
â”œâ”€â”€ data/                      # Data processing
â”‚   â””â”€â”€ data_processor.py      # Data processing utilities
â”œâ”€â”€ training/                  # Training utilities
â”‚   â””â”€â”€ trainer.py             # Training class
â”œâ”€â”€ logs/                      # Training logs
â””â”€â”€ datasets/                  # Dataset files
    â”œâ”€â”€ elderly_command_dataset_reduced.json
    â””â”€â”€ nlp_command_dataset.json
```

## ğŸ”Œ API Endpoints

### ChÃ­nh
- `GET /` - Root endpoint
- `GET /health` - Health check
- `POST /predict` - Predict intent vÃ  extract entities
- `GET /intents` - Láº¥y danh sÃ¡ch intents
- `GET /entities` - Láº¥y entity patterns
- `POST /batch_predict` - Predict nhiá»u texts

### Request Format
```json
{
  "text": "gá»­i tin nháº¯n cho máº¹ ráº±ng tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "confidence_threshold": 0.3
}
```

### Response Format
```json
{
  "input_text": "gá»­i tin nháº¯n cho máº¹ ráº±ng tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "intent": "send-mess",
  "confidence": 0.643,
  "command": "send_message",
  "entities": {
    "RECEIVER": "máº¹",
    "MESSAGE": "tá»‘i con sáº½ vá» Äƒn cÆ¡m"
  },
  "value": "Gá»­i tin nháº¯n cho máº¹: tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "processing_time": 0.123,
  "timestamp": "2024-01-15T10:30:00"
}
```

## ğŸ¯ Intent Types

| Intent | Command | MÃ´ Táº£ |
|--------|---------|-------|
| `call` | `make_call` | Gá»i Ä‘iá»‡n thoáº¡i |
| `send-mess` | `send_message` | Gá»­i tin nháº¯n |
| `set-alarm` | `set_alarm` | Äáº·t bÃ¡o thá»©c |
| `set-reminder` | `set_reminder` | Äáº·t nháº¯c nhá»Ÿ |
| `check-weather` | `check_weather` | Kiá»ƒm tra thá»i tiáº¿t |
| `play-media` | `play_media` | PhÃ¡t nháº¡c/phim |
| `read-news` | `read_news` | Äá»c tin tá»©c |
| `check-health-status` | `check_health` | Kiá»ƒm tra sá»©c khá»e |
| `general-conversation` | `chat` | TrÃ² chuyá»‡n thÃ´ng thÆ°á»ng |

## ğŸ” Entity Types

- **RECEIVER**: NgÆ°á»i nháº­n (bá»‘, máº¹, anh, chá»‹, v.v.)
- **TIME**: Thá»i gian (5 giá», 7:30, sÃ¡ng, chiá»u, v.v.)
- **MESSAGE**: Ná»™i dung tin nháº¯n
- **LOCATION**: Äá»‹a Ä‘iá»ƒm

## âš™ï¸ Cáº¥u HÃ¬nh

### Model Config (`config.py`)
```python
model_size: "base"              # "base" hoáº·c "large"
max_length: 128                 # Äá»™ dÃ i tá»‘i Ä‘a input
batch_size: 8                   # Batch size cho training
learning_rate: 1e-5             # Learning rate
num_epochs: 15                  # Sá»‘ epochs training
```

### Training Config
```python
device: "cpu"                   # "cpu" hoáº·c "cuda"
confidence_threshold: 0.3       # NgÆ°á»¡ng tin cáº­y
```

## ğŸš¨ Troubleshooting

### Lá»—i ThÆ°á»ng Gáº·p

1. **"Model not found"**
   - Cháº¡y `python simple_train.py` Ä‘á»ƒ train model
   - Äáº£m báº£o file `models/best_simple_intent_model.pth` tá»“n táº¡i

2. **"CUDA out of memory"**
   - Giáº£m `batch_size` trong `config.py`
   - Sá»­ dá»¥ng CPU training: `device: "cpu"`

3. **"API connection failed"**
   - Kiá»ƒm tra server Ä‘Ã£ cháº¡y: `python api_server.py`
   - Kiá»ƒm tra port 8000 khÃ´ng bá»‹ chiáº¿m

4. **"Import error"**
   - CÃ i Ä‘áº·t láº¡i dependencies: `pip install -r requirements.txt`
   - Kiá»ƒm tra Python version >= 3.8

### Performance Tips

- **CPU Training**: Giáº£m batch_size, tÄƒng epochs
- **Memory Issues**: Giáº£m max_length, batch_size
- **Speed**: Sá»­ dá»¥ng GPU náº¿u cÃ³

## ğŸ“Š Performance

- **Model Size**: PhoBERT-base (500MB)
- **Training Time**: 30-60 phÃºt (CPU)
- **Inference Time**: ~0.1-0.3s/cÃ¢u
- **Accuracy**: ~74% (validation)

## ğŸ¤ ÄÃ³ng GÃ³p

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push to branch
5. Táº¡o Pull Request

## ğŸ“„ License

MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ™ Acknowledgments

- PhoBERT model tá»« VINAI
- FastAPI framework
- Transformers library tá»« Hugging Face

---

**LÆ°u Ã½**: ÄÃ¢y lÃ  há»‡ thá»‘ng demo, model Ä‘Ã£ Ä‘Æ°á»£c train sáºµn vÃ  sáºµn sÃ ng sá»­ dá»¥ng. Náº¿u cáº§n train láº¡i, hÃ£y lÃ m theo hÆ°á»›ng dáº«n trong pháº§n Training Model.
