# ðŸŽ¯ AUTO NLP - HÆ¯á»šNG DáºªN CHUáº¨N Bá»Š TRAINING DATA

## ðŸ“Š **Tá»”NG QUAN DATASET**

### **Cáº¥u trÃºc dá»¯ liá»‡u hiá»‡n táº¡i:**
```
src/data/
â”œâ”€â”€ raw/                    # Dá»¯ liá»‡u gá»‘c
â”‚   â”œâ”€â”€ *.json             # CÃ¡c file dataset gá»‘c
â”‚   â””â”€â”€ *.md               # Documentation
â”œâ”€â”€ processed/              # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ *.json             # Dataset Ä‘Ã£ clean
â”‚   â””â”€â”€ *.py               # Scripts xá»­ lÃ½
â”œâ”€â”€ grouped/               # Dá»¯ liá»‡u theo nhÃ³m intent
â”‚   â”œâ”€â”€ add-contacts.json  # Intent: thÃªm liÃªn há»‡
â”‚   â”œâ”€â”€ call.json          # Intent: gá»i Ä‘iá»‡n
â”‚   â”œâ”€â”€ send-mess.json     # Intent: nháº¯n tin
â”‚   â””â”€â”€ ...                # CÃ¡c intent khÃ¡c
â””â”€â”€ augmented/             # Dá»¯ liá»‡u má»Ÿ rá»™ng
    â””â”€â”€ expand_dataset.py  # Script táº¡o dá»¯ liá»‡u má»Ÿ rá»™ng
```

## ðŸ”§ **CHUáº¨N Bá»Š Dá»® LIá»†U CHO TRAINING**

### **BÆ°á»›c 1: Kiá»ƒm tra dá»¯ liá»‡u hiá»‡n cÃ³**
```bash
# Kiá»ƒm tra sá»‘ lÆ°á»£ng samples
python -c "
import json
import os
from pathlib import Path

data_dir = Path('src/data/grouped')
total_samples = 0

for file in data_dir.glob('*.json'):
    with open(file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    samples = len(data)
    print(f'{file.name}: {samples} samples')
    total_samples += samples

print(f'Total: {total_samples} samples')
"
```

### **BÆ°á»›c 2: Chuáº©n bá»‹ dataset cho training**
```bash
# Cháº¡y script chuáº©n bá»‹ dá»¯ liá»‡u
python src/data/augmented/expand_dataset.py
```

**Script nÃ y sáº½:**
- âœ… Load táº¥t cáº£ datasets tá»« `src/data/grouped/`
- âœ… Chuáº©n hÃ³a format dá»¯ liá»‡u
- âœ… Táº¡o train/validation split
- âœ… LÆ°u vÃ o `src/data/processed/`

### **BÆ°á»›c 3: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u**
```bash
# Kiá»ƒm tra distribution cá»§a labels
python -c "
import json
from collections import Counter

# Load processed data
with open('src/data/processed/train_dataset.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)

# Count labels
labels = [item['intent'] for item in train_data]
label_counts = Counter(labels)

print('Label distribution:')
for label, count in label_counts.most_common():
    print(f'  {label}: {count} samples')

print(f'Total samples: {len(train_data)}')
print(f'Unique labels: {len(label_counts)}')
"
```

## ðŸŽ¯ **Cáº¤U HÃŒNH TRAINING**

### **File cáº¥u hÃ¬nh: `models/configs/training_config.json`**
```json
{
  "model_name": "vinai/phobert-large",
  "max_length": 128,
  "batch_size": 16,
  "learning_rate": 2e-5,
  "num_epochs": 10,
  "warmup_steps": 500,
  "weight_decay": 0.01,
  "gradient_accumulation_steps": 1,
  "save_steps": 1000,
  "eval_steps": 500,
  "logging_steps": 100,
  "output_dir": "models/trained/",
  "cache_dir": "model_cache/"
}
```

### **TÃ¹y chá»‰nh cáº¥u hÃ¬nh:**
```bash
# Chá»‰nh batch size cho GPU nhá»
python src/training/scripts/train_gpu.py --batch_size 8

# Chá»‰nh learning rate
python src/training/scripts/train_gpu.py --learning_rate 1e-5

# Chá»‰nh sá»‘ epochs
python src/training/scripts/train_gpu.py --num_epochs 15
```

## ðŸš€ **CHáº Y TRAINING**

### **Training cÆ¡ báº£n:**
```bash
# Activate virtual environment
venv_new\Scripts\activate

# Cháº¡y training
python src/training/scripts/train_gpu.py
```

### **Training vá»›i monitoring:**
```bash
# Vá»›i TensorBoard
python src/training/scripts/train_gpu.py --logging_dir models/logs/

# Vá»›i Weights & Biases
python src/training/scripts/train_gpu.py --use_wandb
```

### **Training vá»›i validation:**
```bash
# Cháº¡y vá»›i validation set
python src/training/scripts/train_gpu.py --do_eval --eval_steps 500
```

## ðŸ“ˆ **MONITORING TRAINING**

### **TensorBoard:**
```bash
# Khá»Ÿi Ä‘á»™ng TensorBoard
tensorboard --logdir models/logs/

# Truy cáº­p: http://localhost:6006
```

### **Weights & Biases:**
```bash
# Login W&B
wandb login

# Cháº¡y training vá»›i W&B
python src/training/scripts/train_gpu.py --use_wandb --project_name auto-nlp
```

## ðŸ” **EVALUATION**

### **Test model sau training:**
```bash
# Test trÃªn validation set
python src/training/scripts/train_gpu.py --do_eval --eval_only

# Test vá»›i custom data
python -c "
from src.models.inference.trained_model_inference import TrainedModelInference

# Load trained model
inference = TrainedModelInference('models/trained/best_model')

# Test samples
test_samples = [
    'nháº¯n tin cho máº¹ lÃ  con vá» nhÃ ',
    'gá»i Ä‘iá»‡n cho bá»‘',
    'tÃ¬m kiáº¿m trÃªn google vá» thá»i tiáº¿t'
]

for text in test_samples:
    result = inference.predict(text)
    print(f'Text: {text}')
    print(f'Intent: {result[\"intent\"]} (confidence: {result[\"confidence\"]:.3f})')
    print()
"
```

## ðŸŽ¯ **OPTIMIZATION**

### **Tá»‘i Æ°u cho GPU nhá»:**
```bash
# Giáº£m batch size
python src/training/scripts/train_gpu.py --batch_size 4

# Sá»­ dá»¥ng gradient accumulation
python src/training/scripts/train_gpu.py --gradient_accumulation_steps 4

# Sá»­ dá»¥ng mixed precision
python src/training/scripts/train_gpu.py --fp16
```

### **Tá»‘i Æ°u cho CPU:**
```bash
# Training trÃªn CPU
python src/training/scripts/train_gpu.py --no_cuda

# Giáº£m max_length
python src/training/scripts/train_gpu.py --max_length 64
```

## ðŸ“Š **ANALYSIS**

### **PhÃ¢n tÃ­ch káº¿t quáº£ training:**
```bash
# Xem training logs
python -c "
import json
import matplotlib.pyplot as plt

# Load training logs
with open('models/logs/training_logs.json', 'r') as f:
    logs = json.load(f)

# Plot training curves
epochs = logs['epochs']
train_loss = logs['train_loss']
val_loss = logs['val_loss']

plt.figure(figsize=(10, 6))
plt.plot(epochs, train_loss, label='Train Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training Progress')
plt.savefig('models/logs/training_curves.png')
plt.show()
"
```

## ðŸ”§ **TROUBLESHOOTING**

### **Lá»—i Out of Memory:**
```bash
# Giáº£m batch size
python src/training/scripts/train_gpu.py --batch_size 2

# Sá»­ dá»¥ng gradient checkpointing
python src/training/scripts/train_gpu.py --gradient_checkpointing
```

### **Lá»—i Data Loading:**
```bash
# Kiá»ƒm tra format dá»¯ liá»‡u
python -c "
import json
with open('src/data/processed/train_dataset.json', 'r') as f:
    data = json.load(f)
print('Sample:', data[0])
print('Keys:', data[0].keys())
"
```

### **Lá»—i Model Loading:**
```bash
# Kiá»ƒm tra model cache
ls -la model_cache/

# Clear cache vÃ  táº£i láº¡i
rm -rf model_cache/
python setup_complete.py
```

## ðŸ“‹ **CHECKLIST TRÆ¯á»šC KHI TRAINING**

- [ ] âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ vÃ  clean
- [ ] âœ… Dataset cÃ³ Ä‘á»§ samples cho má»—i intent (>50 samples)
- [ ] âœ… Train/validation split Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] âœ… Model cache Ä‘Ã£ Ä‘Æ°á»£c táº£i
- [ ] âœ… GPU memory Ä‘á»§ cho batch size
- [ ] âœ… Cáº¥u hÃ¬nh training Ä‘Ã£ Ä‘Æ°á»£c set
- [ ] âœ… Logging directory Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] âœ… Output directory Ä‘Ã£ Ä‘Æ°á»£c táº¡o

---
**ðŸŽ‰ ChÃºc báº¡n training thÃ nh cÃ´ng!**
