

## 3.2. Thu thập và Chuẩn hóa dữ liệu

### 3.2.1. Nguồn dữ liệu và lược đồ gán nhãn

- **Nguồn dữ liệu ban đầu**
  - Bộ dữ liệu lệnh giọng nói cho người cao tuổi được lưu trong thư mục `src/data/raw/` dưới nhiều phiên bản kiểm toán và chuẩn hóa (`elderly_command_dataset_*.json`).  
  - Mỗi mẫu dữ liệu bao gồm:
    - Trường `input`: câu lệnh tiếng Việt tự nhiên (có/không dấu, văn nói).  
    - Trường `intent`/`command`: nhãn ý định/loại lệnh thuộc bộ 10 command chuẩn (`call`, `send-mess`, `set-alarm`, `control-device`, …).  
    - Danh sách `entities`/`spans`: các thực thể được gán nhãn theo chuẩn **IOB2** (ví dụ: `CONTACT_NAME`, `RECEIVER`, `PHONE`, `TIME`, `DATE`, `LOCATION`, `MESSAGE`, `DEVICE`, `PLATFORM`, `REMINDER_CONTENT`, …).  
  - Phong cách gán nhãn, quy tắc IOB2 và từ điển gợi ý entity được mô tả chi tiết trong tài liệu `assets/docs/annotation_style_guide.md`.

- **Lược đồ IOB2 và ràng buộc theo tác vụ**
  - Mỗi entity token được đánh nhãn theo:
    - `B-ENTITY`: token đầu của một thực thể.  
    - `I-ENTITY`: token tiếp theo của thực thể đó.  
    - `O`: token không thuộc thực thể.  
  - Từng command có **ràng buộc tối thiểu** về entity, ví dụ:
    - `send-mess`: cần có `RECEIVER`, `MESSAGE` (và thường có `PLATFORM`).  
    - `set-alarm`: cần có ít nhất `TIME` (nếu có `DATE` càng tốt).  
    - `control-device`: cần `DEVICE` và `ACTION`/`MODE` phù hợp.  
    - `search-*`: ưu tiên `QUERY` và/hoặc `PLATFORM`.

### 3.2.2. Chuẩn hóa, làm sạch và đồng bộ dữ liệu

- **Chuẩn hóa mẫu và entity**  
  - Script `scripts/data/normalize_dataset.py` thực hiện:
    - Sửa lỗi chính tả phổ biến trong câu lệnh (ví dụ `"giò"` → `"giờ"`) để tránh tokenization sai.  
    - Chuẩn hóa entity **PLATFORM** cho intent `search-youtube`:
      - Phát hiện các biến thể `"youtube"`, `"you tube"`, `"yt"` và gán lại `PLATFORM = "YouTube"` với `start/end` chính xác.  
    - Đảm bảo mọi mẫu `set-alarm` đều có entity **TIME**:
      - Trích TIME từ chuỗi giờ bằng các regex (định dạng `6h30`, `8:00`, `6 rưỡi`, …).  
      - Nếu không tìm được, sử dụng các từ chỉ thời gian (`"sáng"`, `"trưa"`, `"chiều"`, `"tối"`) hoặc gán mặc định `"08:00"` để không phá vỡ schema.  
    - Chuẩn hóa lại danh sách `entities` bằng `canonicalize_entity_dict`, bổ sung `start/end` nếu thiếu bằng cách dò theo substring trong câu `input`.

- **Sửa lại nhãn BIO sau chuẩn hóa**  
  - Sau bước chỉnh sửa text và entity, script `scripts/data/repair_iob2_labels.py`:
    - Đồng bộ `text/start/end` của từng entity với câu `input` hiện tại.  
    - Gọi `DataProcessor.align_labels(...)` để **tái sinh chuỗi BIO** dựa trên PhoBERT tokenizer, đảm bảo:
      - `entities`, `spans` và `bio_labels` luôn nhất quán.  
      - Các token đặc biệt/padding được đánh `-100` để không ảnh hưởng tới loss trong huấn luyện.

### 3.2.3. Cân bằng lớp và tăng cường dữ liệu

- **Cân bằng theo intent/command**  
  - Script `scripts/data/rebalance_dataset.py` đọc bộ `train_clean.json` và:
    - Đếm số mẫu theo `command`, đặt **mục tiêu số lượng** mỗi intent (`--target`, ví dụ 3300 mẫu/intent).  
    - Nếu một lớp nhiều hơn target → **downsample ngẫu nhiên**.  
    - Nếu ít hơn target → sinh thêm mẫu mới bằng augment (xem dưới) để bù đủ.  
    - Gộp lại, shuffle, rồi ghi ra bộ `train_balanced.json`, đồng thời in thống kê phân phối lớp sau rebalance.

- **Augment cho giọng nói/ASR**  
  - `scripts/augment/augment_vi_asr.py` tạo các biến thể câu lệnh **gần với output của hệ thống nhận dạng giọng nói**:
    - Hoán đổi **số ↔ chữ** cho entity số (`PHONE`, `TIME`, `DATE`, `QUERY`, `FREQUENCY`).  
    - Chèn **filler** (ờ/à/nè/giúp tôi/nhé/ạ/…) vào đầu/cuối hoặc gần động từ chính.  
    - Bỏ **dấu câu**, bỏ **dấu tiếng Việt** một phần/toàn phần ở vùng không phải entity.  
    - Thay **từ đồng nghĩa hẹp** theo ngữ cảnh (`gửi ↔ nhắn`, `bật ↔ mở`, `phát ↔ chạy`, `video ↔ gọi hình`).  
    - Tiêm **lỗi chính tả nhẹ** (1 ký tự) trên token không phải entity.  
  - Sau mỗi biến thể, hệ thống:
    - Tái dựng text + spans bằng hàm `segment_text`/`reassemble_segments`.  
    - Dùng `DataProcessor.align_labels` để sinh lại BIO.  
    - Kiểm tra ràng buộc command qua `validate_required_entities` để loại bỏ biến thể làm mất entity quan trọng.

- **Augment hoán đổi entity (entity swap)**  
  - `scripts/augment/augment_entity_swap.py` sử dụng từ điển `entity_vocab_clean.json` để:
    - Hoán đổi giá trị entity trong cùng một mẫu (ví dụ đổi tên người, địa điểm) nhưng giữ nguyên `intent`/`command`.  
    - Cập nhật lại vị trí `start/end` trong chuỗi `input`.  
    - Loại bỏ mẫu trùng lặp sau augment (`deduplicate_samples`) để giữ dataset đa dạng.

## 3.3. Thiết lập tham số và Quy trình huấn luyện

### 3.3.1. Thiết lập siêu tham số

- **Cấu hình mô hình (ModelConfig)**  
  - `model_name = "vinai/phobert-large"`, `max_length = 128` (tối ưu cho GPU 6GB).  
  - Tham số huấn luyện chính:
    - `num_epochs = 4`, `batch_size = 16`, `learning_rate = 2e-5`, `weight_decay = 0.01`, `warmup_steps = 300`.  
    - `gradient_accumulation_steps = 2`, `max_grad_norm = 1.0`, `use_mixed_precision = True` (AMP).  
  - Trọng số cho hai nhiệm vụ chính:
    - `LAMBDA_INTENT ≈ 0.45`, `LAMBDA_ENTITY ≈ 0.25`, `LAMBDA_COMMAND ≈ 0.2`, kèm lịch warmup riêng cho entity để tránh mất ổn định đầu training.  
  - Hỗ trợ các cơ chế ổn định:
    - Đóng băng encoder PhoBERT một số epoch đầu (`freeze_encoder_epochs`, `freeze_layers`).  
    - Early stopping với `patience` và `min_delta`.  

- **Cấu hình nhãn (IntentConfig, EntityConfig, CommandConfig)**  
  - Bộ intent/command gồm 10 lớp rút gọn, tương ứng trực tiếp với các tác vụ của hệ thống.  
  - Nhãn entity được sinh tự động từ `ENTITY_BASE_NAMES` qua `generate_entity_labels()` để đảm bảo bao phủ đầy đủ các biến thể `B-*/I-*` theo IOB2.

### 3.3.2. Chuẩn bị dữ liệu huấn luyện đa nhiệm

- **Tiền xử lý với DataProcessor**  
  - `src/data/processed/data_processor.py` (`DataProcessor`) đảm nhiệm:
    - Tokenize câu lệnh bằng tokenizer PhoBERT, sinh `input_ids` và `attention_mask`.  
    - Căn chỉnh entity spans → BIO → ID với `align_labels`, đảm bảo phù hợp với chuỗi token (bỏ qua special tokens/padding bằng `-100`).  
    - Map:
      - `intent` → `intent_label` (theo `IntentConfig.intent_labels`).  
      - `command` (kebab-case) → `command_label` (theo `CommandConfig.command_labels`).  
  - Hỗ trợ hai chế độ chuẩn bị dữ liệu:
    - Dựa trên nhãn dataset (`prepare_multi_task_data`): dùng `entities/spans` đã gán nhãn.  
    - Dựa trên rule-based (`prepare_multi_task_data_rule_based`): tự trích một số entity quan trọng (như `RECEIVER`, `PLATFORM`, `TIME`) bằng regex khi thiếu span.

- **Xử lý mất cân bằng lớp trong training**  
  - `ImbalancedDatasetProcessor` (`src/data/processed/dataset_processor.py`) hỗ trợ:
    - Phân tích phân phối lớp, augment thêm cho lớp thiểu số bằng các chiến lược nhẹ (thay từ đồng nghĩa, đảo thứ tự từ, thêm filler, paraphrase).  
    - Dùng `WeightedRandomSampler` trong DataLoader để tăng xác suất chọn mẫu thuộc lớp ít xuất hiện.  
    - Chia train/val theo **stratified split** để giữ phân phối lớp giữa các tập.

### 3.3.3. Quy trình huấn luyện MultiTaskModel

- **Khởi tạo mô hình và trainer**  
  - Tạo `MultiTaskModel` (PhoBERT encoder + 3 head intent/command/entity) với dropout theo config.  
  - Sử dụng lớp `MultitaskTrainer` (`src/training/pipeline/trainer.py`) để:
    - Thiết lập optimizer `AdamW` và scheduler `get_linear_schedule_with_warmup`.  
    - Khởi tạo loss cho từng nhiệm vụ (CrossEntropy) và kết hợp theo trọng số đã nêu.  
    - Bật AMP (`torch.amp.GradScaler`) khi có CUDA, áp dụng gradient clipping (`torch.nn.utils.clip_grad_norm_`).  

- **Vòng lặp huấn luyện – đánh giá – lưu mô hình**  
  - Với mỗi epoch:
    - `_train_one_epoch`: duyệt qua DataLoader train, tích lũy loss, cập nhật optimizer/scheduler sau số bước `gradient_accumulation_steps` chỉ định.  
    - `evaluate`: tính toán các metric:
      - Intent/command: accuracy, macro/weighted F1, precision, recall.  
      - Entity: precision/recall/F1 theo seqeval, tỉ lệ token non-O.  
    - Tính điểm tổng hợp val dựa trên ba nhóm nhiệm vụ, chọn checkpoint tốt nhất (`best_model.pt`) và lưu thêm checkpoint từng epoch (`checkpoint-epoch*.pt`).  
  - Sau cùng, có thể đánh giá trên tập test và sinh báo cáo chi tiết trong thư mục `reports/` (benchmark, training dynamics, evaluation flow).

## 3.4. Tối ưu hiệu suất suy luận

### 3.4.1. Tối ưu mô hình và hạ tầng runtime

- **Tối ưu ở tầng mô hình**
  - Sử dụng **PhoBERT-large** với `max_length = 128` và một kiến trúc multi-task gọn (3 head tuyến tính) để giữ latency thấp.  
  - Trong suy luận, mô hình được bọc bởi `MultiTaskInference`/`TrainedModelInference`, sử dụng `torch.no_grad()` để giảm chi phí bộ nhớ và tăng tốc.  
  - Trọng số tốt nhất (`best_model`) được load một lần duy nhất khi khởi động API, sau đó tái sử dụng cho mọi request.  

- **Tối ưu ở tầng API (FastAPI + Uvicorn)**
  - File `api/server.py` khởi tạo `ModelFirstHybridSystem` trong sự kiện `startup` và lưu trong biến toàn cục `hybrid_system`.  
  - Các endpoint `/predict` và `/predict-simple`:
    - Nhận JSON gọn (`text`, `context` tùy chọn), trả về `intent`, `command`, `entities`, `confidence` và `processing_time`.  
    - Endpoint `predict-simple` còn tối giản hóa phần `entities` thành chuỗi để giảm kích thước payload.  
  - Endpoint `/health`, `/stats` và `/test` cho phép:
    - Theo dõi `avg_processing_time`, `success_rate`, số lần fallback, … để giám sát hiệu năng và chất lượng suy luận theo thời gian.

### 3.4.2. Tối ưu tầng Reasoning và Hybrid decision

- **Caching và vector store**  
  - `ReasoningCache` lưu:
    - Embedding PhoBERT của các câu đã xử lý.  
    - Kết quả similarity và reasoning, giúp các lần gọi lại cùng câu (hoặc câu gần) gần như không tốn chi phí tính toán lại.  
  - `VectorStore` (FAISS) chỉ index các câu synonym intent từ `knowledge_base.json`, cho phép truy vấn **ngữ nghĩa gần đúng** với độ phức tạp thấp và có thể tắt/bật qua config (khi tài nguyên hạn chế).  

- **Chiến lược hybrid model-first**  
  - Trong `ModelFirstHybridSystem._hybrid_decision`:
    - Đặt các ngưỡng confidence để chọn giữa **model**, **reasoning** hoặc **kết hợp**.  
    - Chỉ sử dụng reasoning như một “lớp tăng cường/kiểm tra” khi model chưa đủ chắc chắn, hạn chế chi phí suy luận.  
    - Áp dụng nhiều **heuristic rẻ** (chạy trên chuỗi text lower-case) để override nhanh các intent đặc biệt (video call, set-alarm, search-youtube, control-device/wifi/âm lượng/độ sáng, add-contacts, search-internet).  
  - Hậu xử lý entity trong `_postprocess_command_entities` được thiết kế nhẹ, chỉ gồm:
    - Các phép kiểm tra/chuẩn hóa chuỗi đơn giản.  
    - Mapping command → tập entity cần giữ, loại bỏ entity thừa và, khi cần, bổ sung entity còn thiếu qua `SpecializedEntityExtractor`.

### 3.4.3. Theo dõi và điều chỉnh hiệu năng

- **Thống kê nội bộ trong Hybrid System**
  - `ModelFirstHybridSystem` duy trì biến `stats`:
    - `total_predictions`, `model_predictions`, `reasoning_predictions`, `hybrid_predictions`, `fallback_predictions`.  
    - `avg_processing_time`, danh sách confidence gần đây để tính `avg/min/max_confidence`.  
    - `success_rate` (tỉ lệ request không rơi vào fallback).  
  - Các thống kê này được xuất qua endpoint `/stats` để:
    - Xác định liệu reasoning có đang bị gọi quá nhiều (dấu hiệu model chưa tốt hoặc threshold chưa hợp lý).  
    - Đánh giá ảnh hưởng của các thay đổi tham số (ví dụ tăng/giảm `CONFIDENCE_THRESHOLD`, bật/tắt specialized entity extraction, …) tới hiệu suất suy luận.

- **Cấu hình linh hoạt theo môi trường triển khai**
  - Thông qua `config.py` và biến môi trường, hệ thống có thể:
    - Điều chỉnh `MODEL_NAME`, `BATCH_SIZE`, `MAX_LENGTH`, `USE_MIXED_PRECISION`, … tùy theo cấu hình GPU/CPU thực tế.  
    - Thay đổi cổng, host API (`API_HOST`, `API_PORT`) và chế độ debug để phù hợp với môi trường phát triển/production.  

