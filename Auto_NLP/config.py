import os
from dataclasses import dataclass
from typing import List

class ModelConfig:
    """Cáº¥u hÃ¬nh tá»‘i Æ°u cho Model vá»›i cáº£i tiáº¿n cho PhoBERT-Large"""
    
    def __init__(self):
        # CÃ i Ä‘áº·t cÆ¡ báº£n
        self.model_name = "vinai/phobert-large"
        self.model_size = "large"  # "base" hoáº·c "large"
        self.max_length = 256  # Giáº£m tá»« 512 Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›
        
        # GPU settings
        import torch
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.use_fp16 = True  # Sá»­ dá»¥ng mixed precision
        self.use_amp = True  # Automatic Mixed Precision
        self.gradient_checkpointing = True  # Tiáº¿t kiá»‡m memory cho large model
        self.use_mixed_precision = True  # Sá»­ dá»¥ng mixed precision Ä‘á»ƒ tiáº¿t kiá»‡m memory
        
        # Optimizer settings
        self.batch_size = 16  # Giáº£m batch size cho large model
        self.gradient_accumulation_steps = 2  # TÄƒng lÃªn 2-4 Ä‘á»ƒ mÃ´ phá»ng batch size lá»›n hÆ¡n
        self.learning_rate = 2e-5
        self.weight_decay = 0.01
        self.adam_epsilon = 1e-8
        self.max_grad_norm = 1.0
        self.warmup_steps = 500
        
        # Training settings
        self.num_epochs = 10
        self.num_workers = 4
        self.pin_memory = True
        
        # Model architecture
        self.dropout = 0.2
        self.freeze_layers = 6  # ÄÃ³ng bÄƒng 6 layer Ä‘áº§u tiÃªn
        
        # Optimizer type
        self.optimizer = "adamw"  # "adamw" hoáº·c "adafactor"
        
        # Äiá»u chá»‰nh cáº¥u hÃ¬nh dá»±a trÃªn kÃ­ch thÆ°á»›c model
        if self.model_size == "large":
            self.batch_size = 8  # Giáº£m batch size cho large model
            self.gradient_accumulation_steps = 4
            self.max_length = 192  # Tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n
            self.learning_rate = 1e-5  # Giáº£m learning rate
            self.num_epochs = 15  # TÄƒng epochs Ä‘á»ƒ táº­n dá»¥ng large model
            self.warmup_steps = 300  # TÄƒng warmup steps cho large model
            self.dropout = 0.1  # Giáº£m dropout Ä‘á»ƒ táº­n dá»¥ng capacity cá»§a large model
            self.freeze_layers = 8  # ÄÃ³ng bÄƒng nhiá»u layer hÆ¡n cho large model
            self.use_fp16 = False  # Táº¯t FP16 Ä‘á»ƒ trÃ¡nh lá»—i gradients
    
    @property
    def hidden_size(self) -> int:
        """Láº¥y hidden size dá»±a trÃªn model size"""
        if self.model_size == "large":
            return 1024
        else:
            return 768
    
    @property
    def num_layers(self) -> int:
        """Láº¥y sá»‘ layers dá»±a trÃªn model size"""
        if self.model_size == "large":
            return 24
        else:
            return 12
    
    @property
    def num_attention_heads(self) -> int:
        """Láº¥y sá»‘ attention heads dá»±a trÃªn model size"""
        if self.model_size == "large":
            return 16
        else:
            return 12
    
    def __post_init__(self):
        """Validate model size vÃ  setup device"""
        if self.model_size not in ["base", "large"]:
            raise ValueError("model_size pháº£i lÃ  'base' hoáº·c 'large'")
        
        # Auto-detect device
        if self.device == "auto":
            import torch
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ðŸ–¥ï¸ Auto-detected device: {self.device}")
            
            if self.device == "cuda":
                print(f"ðŸŽ® GPU: {torch.cuda.get_device_name()}")
                print(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def __post_init__(self):
        """Validate model size vÃ  setup device"""
        if self.model_size not in ["base", "large"]:
            raise ValueError("model_size pháº£i lÃ  'base' hoáº·c 'large'")
        
        # Auto-detect device
        if self.device == "auto":
            import torch
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ðŸ–¥ï¸ Auto-detected device: {self.device}")
            
            if self.device == "cuda":
                print(f"ðŸŽ® GPU: {torch.cuda.get_device_name()}")
                print(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

@dataclass
class IntentConfig:
    """Cáº¥u hÃ¬nh cho Intent Recognition nÃ¢ng cao"""
    num_intents: int = 28  # Cáº­p nháº­t theo dataset thá»±c táº¿: 27 commands
    intent_labels: List[str] = None
    
    # Confidence threshold cho intent recognition
    confidence_threshold: float = 0.7
    unknown_intent_label: str = "unknown"
    
    # Attention mechanism settings
    use_attention: bool = True
    num_attention_heads: int = 8
    attention_dropout: float = 0.1
    
    # Multi-layer classifier settings
    use_multi_layer: bool = True
    hidden_layer_sizes: List[int] = None  # Sáº½ Ä‘Æ°á»£c set Ä‘á»™ng dá»±a trÃªn model size
    
    # CRF settings
    use_crf: bool = False
    
    # Ensemble settings
    use_ensemble: bool = False
    ensemble_method: str = "weighted"  # "weighted", "voting", "stacking"
    
    # Advanced features
    use_confidence_scoring: bool = True
    use_batch_norm: bool = True
    use_residual_connections: bool = True
    
    def __post_init__(self):
        if self.intent_labels is None:
            # Cáº­p nháº­t theo dataset thá»±c táº¿: 27 commands
            self.intent_labels = [
                "adjust-settings", "app-tutorial", "browse-social-media", "call", 
                "check-device-status", "check-health-status", "check-messages", 
                "check-weather", "control-device", "general-conversation", 
                "make-call", "make-video-call", "navigation-help", "open-app", 
                "open-app-action", "play-audio", "play-content", "play-media", 
                "provide-instructions", "read-content", "read-news", "search-content", 
                "search-internet", "send-message", "send-mess", "set-alarm", 
                "set-reminder", "view-content"
            ]
        
        # Set hidden layer sizes dá»±a trÃªn model size
        if self.hidden_layer_sizes is None:
            # Sá»­ dá»¥ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh thay vÃ¬ relative import
            base_size = 768  # PhoBERT-base hidden size
            self.hidden_layer_sizes = [base_size, base_size // 2, base_size // 4]

@dataclass
class EntityConfig:
    """Cáº¥u hÃ¬nh cho Entity Extraction - Cáº­p nháº­t theo dataset má»›i"""
    entity_labels: List[str] = None
    
    def __post_init__(self):
        if self.entity_labels is None:
            # Cáº­p nháº­t theo dataset má»›i: 6 entity labels chÃ­nh
            self.entity_labels = [
                "O",  # Outside
                "FAMILY_RELATIONSHIP",  # Má»‘i quan há»‡ gia Ä‘Ã¬nh
                "DATE_EXPRESSION",      # Biá»ƒu thá»©c ngÃ y thÃ¡ng
                "CONTACT_PERSON",       # NgÆ°á»i liÃªn há»‡
                "LOCATION",             # Äá»‹a Ä‘iá»ƒm
                "TIME_EXPRESSION",      # Biá»ƒu thá»©c thá»i gian
                "ARTIST_NAME"           # TÃªn nghá»‡ sÄ©
            ]
    
    @property
    def num_entities(self) -> int:
        return len(self.entity_labels)

@dataclass
class ValueConfig:
    """Cáº¥u hÃ¬nh cho Value Extraction - Má»›i thÃªm"""
    value_labels: List[str] = None
    
    def __post_init__(self):
        if self.value_labels is None:
            # Cáº­p nháº­t theo dataset má»›i: 14 value labels chÃ­nh
            self.value_labels = [
                "TIME_EXPRESSION",      # Biá»ƒu thá»©c thá»i gian
                "MESSAGE_CONTENT",      # Ná»™i dung tin nháº¯n
                "REMINDER_CONTENT",     # Ná»™i dung nháº¯c nhá»Ÿ
                "HEALTH_METRIC",        # Chá»‰ sá»‘ sá»©c khá»e
                "MEDIA_TYPE",           # Loáº¡i media
                "NEWS_CATEGORY",        # Danh má»¥c tin tá»©c
                "DATE_EXPRESSION",      # Biá»ƒu thá»©c ngÃ y thÃ¡ng
                "WEATHER_CONDITION",    # Äiá»u kiá»‡n thá»i tiáº¿t
                "MEDIA_CONTENT",        # Ná»™i dung media
                "CALL_TYPE",            # Loáº¡i cuá»™c gá»i
                "FREQUENCY",            # Táº§n suáº¥t
                "SYMPTOM",              # Triá»‡u chá»©ng
                "TOPIC_NEWS",           # Chá»§ Ä‘á» tin tá»©c
                "NEWS_SOURCE"           # Nguá»“n tin tá»©c
            ]
    
    @property
    def num_values(self) -> int:
        return len(self.value_labels)

@dataclass
class CommandConfig:
    """Cáº¥u hÃ¬nh cho Command Processing - Cáº­p nháº­t theo dataset thá»±c táº¿"""
    command_labels: List[str] = None
    
    def __post_init__(self):
        if self.command_labels is None:
            # Cáº­p nháº­t theo dataset thá»±c táº¿: 28 commands
            self.command_labels = [
                "adjust-settings", "app-tutorial", "browse-social-media", "call", 
                "check-device-status", "check-health-status", "check-messages", 
                "check-weather", "control-device", "general-conversation", 
                "make-call", "make-video-call", "navigation-help", "open-app", 
                "open-app-action", "play-audio", "play-content", "play-media", 
                "provide-instructions", "read-content", "read-news", "search-content", 
                "search-internet", "send-mess", "send-message", "set-alarm", 
                "set-reminder", "view-content", "unknown"
            ]
    
    @property
    def num_commands(self) -> int:
        return len(self.command_labels)

@dataclass
class TrainingConfig:
    """Cáº¥u hÃ¬nh cho training - Tá»‘i Æ°u cho GPU vÃ  Large model"""
    output_dir: str = "./models"
    data_dir: str = "./data"
    log_dir: str = "./logs"
    seed: int = 42
    device: str = "auto"  # Auto-detect GPU/CPU
    save_steps: int = 50  # TÄƒng save frequency cho large model
    eval_steps: int = 50  # TÄƒng eval frequency cho large model
    logging_steps: int = 10  # TÄƒng logging frequency Ä‘á»ƒ theo dÃµi large model
    
    # GPU optimization settings
    use_mixed_precision: bool = True  # Báº­t mixed precision cho GPU
    max_grad_norm: float = 1.0  # Gradient clipping
    early_stopping_patience: int = 15  # TÄƒng patience cho large model
    save_best_only: bool = False  # LÆ°u táº¥t cáº£ checkpoints Ä‘á»ƒ cÃ³ thá»ƒ resume
    save_total_limit: int = 10  # Giá»¯ 10 checkpoints gáº§n nháº¥t
    
    # Large model specific settings
    use_gradient_checkpointing: bool = True  # Tiáº¿t kiá»‡m memory
    use_fp16: bool = True  # Sá»­ dá»¥ng FP16
    use_amp: bool = True  # Automatic Mixed Precision
    max_memory_usage: str = "auto"  # Auto-detect memory usage
    
    def __post_init__(self):
        # Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)

# Khá»Ÿi táº¡o cÃ¡c config
model_config = ModelConfig()
intent_config = IntentConfig()
entity_config = EntityConfig()
value_config = ValueConfig()
command_config = CommandConfig()
training_config = TrainingConfig()

# In thÃ´ng tin cáº¥u hÃ¬nh
print("ðŸ”§ Model Configuration:")
print(f"  Model: {model_config.model_name}")
print(f"  Size: {model_config.model_size}")
print(f"  Max Length: {model_config.max_length}")
print(f"  Batch Size: {model_config.batch_size}")
print(f"  Learning Rate: {model_config.learning_rate}")
print(f"  Epochs: {model_config.num_epochs}")
print(f"  Device: {model_config.device}")
print(f"  FP16: {model_config.use_fp16}")
print(f"  Gradient Checkpointing: {model_config.gradient_checkpointing}")
print(f"  Freeze Layers: {model_config.freeze_layers}")
print(f"  Optimizer: {model_config.optimizer}") 