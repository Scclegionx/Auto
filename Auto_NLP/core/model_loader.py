from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple

from models.inference.model_loader import MultiTaskInference, _select_checkpoint


class TrainedModelInference:
    """Compatibility wrapper s·ª≠ d·ª•ng `MultiTaskInference` cho core hybrid system."""

    def __init__(self, model_path: str, _device: Optional[str] = None):
        model_dir = Path(model_path)
        if model_dir.is_file():
            checkpoint_path = model_dir
            model_dir = model_dir.parent
        else:
            if not model_dir.exists():
                raise FileNotFoundError(f"Model directory kh√¥ng t·ªìn t·∫°i: {model_dir}")
            checkpoint_path = _select_checkpoint(model_dir)

        tokenizer_path = str(model_dir)
        config_path = model_dir / "config.json"
        if not config_path.exists():
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y config.json t·∫°i {config_path}")

        self._inference = MultiTaskInference(str(checkpoint_path), tokenizer_path, str(config_path))
        self.model_loaded = True
        self.model_path = model_dir

    def predict(self, text: str) -> Dict[str, Any]:
        """
        G·ªçi multi-task model v√† h·∫≠u x·ª≠ l√Ω entity cho hybrid system.
        - Lo·∫°i b·ªè entity r√°c t·ª´ special tokens (<s>, </s>, [PAD], ...)
        - Gom MESSAGE th√†nh m·ªôt chu·ªói duy nh·∫•t (gh√©p c√°c m·∫£nh)
        - L·ªçc PLATFORM theo whitelist (zalo, messenger, viber, youtube, ...)
        """
        result = self._inference.predict(text)

        raw_entities: List[Dict[str, Any]] = result.get("entities", [])
        cleaned_entities: List[Tuple[str, str]] = []

        special_tokens = {"<s>", "</s>", "<pad>", "<unk>", "[PAD]", "[CLS]", "[SEP]"}
        platform_whitelist = {
            "zalo",
            "messenger",
            "facebook",
            "fb",
            "viber",
            "youtube",
            "zalo call",
            "zalo video",
        }

        for ent in raw_entities:
            label = ent.get("label")
            text_value = (ent.get("text") or "").strip()
            if not label or not text_value:
                continue
            if text_value in special_tokens:
                continue

            # L·ªçc PLATFORM: ch·ªâ gi·ªØ n·∫øu kh·ªõp whitelist (b·ªè c√°c m·∫£nh nh∆∞ "tin", "nh·∫Øn"...)
            if label == "PLATFORM":
                low = text_value.lower().strip()
                compact = low.replace(" ", "")
                if compact not in {p.replace(" ", "") for p in platform_whitelist}:
                    continue

            cleaned_entities.append((label, text_value))

        entity_map: Dict[str, Any] = {}
        message_pieces: List[str] = []
        query_pieces: List[str] = []

        # M·ªôt s·ªë label c√≥ th·ªÉ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn (MESSAGE, QUERY, ...),
        # c√°c label kh√°c th√¨ ∆∞u ti√™n:
        # - n·∫øu ch·ªâ c√≥ 1 span: d√πng span ƒë√≥
        # - n·∫øu c√≥ nhi·ªÅu span: gi·ªØ span "t·ªët" h∆°n (d√†i h∆°n, kh√¥ng ph·∫£i trigger nh∆∞ "b·∫≠t", "m·ªü", ...)
        trigger_verbs = {"b·∫≠t", "t·∫Øt", "m·ªü", "gi·∫£m", "tƒÉng", "t√¨m", "tra", "h·ªèi"}

        for label, text_value in cleaned_entities:
            if label == "MESSAGE":
                message_pieces.append(text_value)
                continue

            if label == "QUERY":
                query_pieces.append(text_value)
                continue

            # C√°c label kh√°c: DEVICE, TIME, DATE, PLATFORM, ...
            existing = entity_map.get(label)
            if existing is None:
                entity_map[label] = text_value
                continue

            # N·∫øu ƒë√£ c√≥ value cho label n√†y, ch·ªçn value "t·ªët" h∆°n:
            # - ∆Øu ti√™n span d√†i h∆°n r√µ r√†ng h∆°n (v√≠ d·ª• "ƒëi·ªÅu h√≤a 26 ƒë·ªô" thay cho "b·∫≠t")
            # - N·∫øu existing l√† trigger verb (b·∫≠t/t·∫Øt/m·ªü/...) v√† text_value d√†i h∆°n => thay th·∫ø
            existing_len = len(existing)
            new_len = len(text_value)

            if existing in trigger_verbs and new_len > existing_len:
                entity_map[label] = text_value
            elif new_len > existing_len and label in {"DEVICE"}:
                entity_map[label] = text_value
            # C√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i: gi·ªØ nguy√™n existing ƒë·ªÉ tr√°nh thay ƒë·ªïi qu√° m·∫°nh

        if message_pieces:
            merged = " ".join(message_pieces)
            merged = " ".join(merged.split())
            entity_map["MESSAGE"] = merged

        if query_pieces:
            # N·∫øu c√≥ nhi·ªÅu m·∫£nh QUERY, b·ªè b·ªõt c√°c trigger verb ƒë·ª©ng ri√™ng l·∫ª nh∆∞ "m·ªü", "t√¨m", "tra"
            # v√† ∆∞u ti√™n ph·∫ßn n·ªôi dung ch√≠nh.
            filtered: List[str] = []
            for q in query_pieces:
                if q in trigger_verbs and len(query_pieces) > 1:
                    continue
                filtered.append(q)
            if not filtered:
                filtered = query_pieces
            merged_query = " ".join(filtered)
            merged_query = " ".join(merged_query.split())
            entity_map["QUERY"] = merged_query

        return {
            "intent": result.get("intent", "unknown"),
            "confidence": result.get("intent_confidence", 0.0),
            "entities": entity_map,
            "command": result.get("command", result.get("intent", "unknown")),
            "model_type": "multi-task",
        }

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "model_loaded": self.model_loaded,
            "model_path": str(self.model_path),
        }

def load_trained_model(model_name: str = "phobert_multitask", device: Optional[torch.device] = None) -> TrainedModelInference:
    """
    Load trained model
    
    Args:
        model_name: Name of the model directory
        device: Device to load model on
        
    Returns:
        TrainedModelInference instance
    """
    model_path = Path("models") / model_name
    return TrainedModelInference(str(model_path), device)

# Test function
if __name__ == "__main__":
    print("üöÄ Testing TrainedModelInference...")
    
    try:
        # Load model
        model = load_trained_model("phobert_large_intent_model")
        
        # Test cases
        test_cases = [
            "g·ªçi ƒëi·ªán cho m·∫π",
            "b·∫≠t ƒë√®n ph√≤ng kh√°ch",
            "t√¨m ki·∫øm nh·∫°c tr√™n youtube",
            "ƒë·∫∑t b√°o th·ª©c 7 gi·ªù s√°ng",
            "g·ª≠i tin nh·∫Øn cho b·∫°n"
        ]
        
        print(f"\nüß™ Testing with {len(test_cases)} test cases...")
        
        for i, test_case in enumerate(test_cases):
            print(f"\n{i+1}. Testing: '{test_case}'")
            result = model.predict(test_case)
            print(f"   Intent: {result['intent']}")
            print(f"   Confidence: {result['confidence']:.3f}")
            print(f"   Command: {result['command']}")
            print(f"   Entities: {result['entities']}")
            print(f"   Model type: {result['model_type']}")
        
        # Print model info
        print(f"\nüìä Model Info:")
        info = model.get_model_info()
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        print(f"\n‚úÖ TrainedModelInference test completed!")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
