#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thá»‘ng kÃª Ä‘áº§y Ä‘á»§ master dataset.
Usage:
    python scripts/data/analyze_master_dataset.py --input src/data/raw/master_dataset_35609.json
"""

from __future__ import annotations

import argparse
import json
from collections import Counter
from pathlib import Path
from typing import Dict, List


def load_dataset(path: Path) -> List[Dict]:
    """Load dataset tá»« file JSON."""
    print(f"Äang Ä‘á»c: {path}")
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError(f"Dataset pháº£i lÃ  list, nháº­n Ä‘Æ°á»£c: {type(data)}")
    return data


def analyze_dataset(samples: List[Dict]) -> Dict:
    """PhÃ¢n tÃ­ch toÃ n diá»‡n dataset."""
    print("\n" + "=" * 60)
    print("THá»NG KÃŠ DATASET")
    print("=" * 60)
    
    # 1. Tá»•ng sá»‘ máº«u
    total = len(samples)
    print(f"\nğŸ“Š Tá»•ng sá»‘ máº«u: {total:,}")
    
    # 2. PhÃ¢n bá»‘ Intent
    intent_counter = Counter()
    command_counter = Counter()
    
    for sample in samples:
        intent = sample.get("intent", sample.get("command", "unknown"))
        command = sample.get("command", sample.get("intent", "unknown"))
        intent_counter[intent] += 1
        command_counter[command] += 1
    
    print(f"\nğŸ“Œ PhÃ¢n bá»‘ Intent ({len(intent_counter)} loáº¡i):")
    for intent, count in intent_counter.most_common():
        pct = count / total * 100
        print(f"  {intent:20s}: {count:6,} máº«u ({pct:5.2f}%)")
    
    print(f"\nğŸ“Œ PhÃ¢n bá»‘ Command ({len(command_counter)} loáº¡i):")
    for command, count in command_counter.most_common():
        pct = count / total * 100
        print(f"  {command:20s}: {count:6,} máº«u ({pct:5.2f}%)")
    
    # 3. Thá»‘ng kÃª Entity
    entity_label_counter = Counter()
    total_entities = 0
    samples_with_entities = 0
    
    for sample in samples:
        entities = sample.get("entities", [])
        if entities:
            samples_with_entities += 1
            for ent in entities:
                if isinstance(ent, dict):
                    label = ent.get("label", "UNKNOWN")
                    entity_label_counter[label] += 1
                    total_entities += 1
    
    print(f"\nğŸ“Œ Thá»‘ng kÃª Entity:")
    print(f"  Tá»•ng sá»‘ entity: {total_entities:,}")
    print(f"  Máº«u cÃ³ entity: {samples_with_entities:,} / {total:,} ({samples_with_entities/total*100:.2f}%)")
    print(f"  Trung bÃ¬nh entity/máº«u: {total_entities/total:.2f}")
    
    print(f"\n  Top 15 entity labels:")
    for label, count in entity_label_counter.most_common(15):
        pct = count / total_entities * 100
        print(f"    {label:20s}: {count:6,} ({pct:5.2f}%)")
    
    # 4. Äá»™ dÃ i cÃ¢u (input)
    lengths = []
    for sample in samples:
        text = sample.get("input", "")
        lengths.append(len(text.split()))
    
    if lengths:
        avg_len = sum(lengths) / len(lengths)
        min_len = min(lengths)
        max_len = max(lengths)
        
        print(f"\nğŸ“Œ Äá»™ dÃ i cÃ¢u (sá»‘ tá»«):")
        print(f"  Trung bÃ¬nh: {avg_len:.2f}")
        print(f"  Min: {min_len}")
        print(f"  Max: {max_len}")
    
    # 5. Thá»‘ng kÃª BIO labels (náº¿u cÃ³)
    bio_counter = Counter()
    samples_with_bio = 0
    
    for sample in samples:
        bio_labels = sample.get("bio_labels", [])
        if bio_labels:
            samples_with_bio += 1
            for label in bio_labels:
                bio_counter[label] += 1
    
    print(f"\nğŸ“Œ Thá»‘ng kÃª BIO labels:")
    print(f"  Máº«u cÃ³ bio_labels: {samples_with_bio:,} / {total:,} ({samples_with_bio/total*100:.2f}%)")
    if bio_counter:
        print(f"  Top 10 BIO tags:")
        for label, count in bio_counter.most_common(10):
            print(f"    {label:20s}: {count:6,}")
    
    # 6. Thá»‘ng kÃª split (náº¿u cÃ³)
    split_counter = Counter()
    for sample in samples:
        split = sample.get("split", "unknown")
        split_counter[split] += 1
    
    if len(split_counter) > 1:
        print(f"\nğŸ“Œ PhÃ¢n bá»‘ Split:")
        for split, count in split_counter.most_common():
            pct = count / total * 100
            print(f"  {split:15s}: {count:6,} máº«u ({pct:5.2f}%)")
    
    print(f"\n{'='*60}")
    print("âœ… PhÃ¢n tÃ­ch hoÃ n táº¥t!")
    print("=" * 60)
    
    return {
        "total_samples": total,
        "intent_distribution": dict(intent_counter),
        "command_distribution": dict(command_counter),
        "entity_stats": {
            "total_entities": total_entities,
            "samples_with_entities": samples_with_entities,
            "label_distribution": dict(entity_label_counter.most_common(20)),
        },
        "text_length": {
            "avg": avg_len if lengths else 0,
            "min": min_len if lengths else 0,
            "max": max_len if lengths else 0,
        },
    }


def main():
    parser = argparse.ArgumentParser(description="PhÃ¢n tÃ­ch master dataset.")
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("src/data/raw/master_dataset_35609.json"),
        help="ÄÆ°á»ng dáº«n tá»›i master dataset.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="ÄÆ°á»ng dáº«n lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch (JSON), náº¿u muá»‘n.",
    )
    args = parser.parse_args()
    
    samples = load_dataset(args.input)
    stats = analyze_dataset(samples)
    
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with args.output.open("w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch táº¡i: {args.output}")


if __name__ == "__main__":
    main()





