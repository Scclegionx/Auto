import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
import re
from datetime import datetime
import json
import os
import sys
import os
from pathlib import Path

# Add project root to path
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.insert(0, str(project_root))

# Import NLP processor with absolute import
try:
    from src.inference.engines.nlp_processor import NLPProcessor
except ImportError:
    # Fallback for direct execution
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
    from src.inference.engines.nlp_processor import NLPProcessor

try:
    from src.training.configs.config import model_config
    from src.inference.engines.reasoning_engine import ReasoningEngine
except ImportError as e:
    print(f"Import error: {e}")
    print("üîß Trying alternative import...")
    
    # Alternative import path
    sys.path.insert(0, str(project_root / "src"))
    from training.configs.config import model_config
    from inference.engines.reasoning_engine import ReasoningEngine

class IntentRequest(BaseModel):
    text: str
    confidence_threshold: Optional[float] = 0.3

class IntentResponse(BaseModel):
    input_text: str
    intent: str
    confidence: float
    command: str
    entities: Dict[str, str]
    value: str
    processing_time: float
    timestamp: str

class SimpleIntentModel(nn.Module):
    """Model t·ªëi ∆∞u cho Intent Recognition v·ªõi Large model v√† GPU"""
    
    def __init__(self, model_name, num_intents, config):
        super().__init__()
        self.config = config
        
        self.phobert = AutoModel.from_pretrained(
            model_name,
            gradient_checkpointing=config.gradient_checkpointing
        )
        
        hidden_size = self.phobert.config.hidden_size
        
        if config.model_size == "large":
            self.classifier = nn.Sequential(
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size // 2),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size // 2, hidden_size // 4),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size // 4),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size // 4, num_intents)
            )
        else:
            self.classifier = nn.Sequential(
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size, num_intents)
            )
    
    def forward(self, input_ids, attention_mask):
        if self.config.gradient_checkpointing and self.training:
            outputs = self.phobert(
                input_ids=input_ids, 
                attention_mask=attention_mask,
                use_cache=False  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám memory
            )
        else:
            outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        
        sequence_output = outputs.last_hidden_state
        attention_mask_expanded = attention_mask.unsqueeze(-1).float()
        pooled_output = (sequence_output * attention_mask_expanded).sum(dim=1) / attention_mask_expanded.sum(dim=1)
        
        logits = self.classifier(pooled_output)
        return logits

class PhoBERT_SAM_API:
    """API class cho PhoBERT_SAM"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        if self.device.type == "cuda":
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # Initialize NLP processor
        self.nlp_processor = NLPProcessor(self.device)
        
        # Legacy attributes for backward compatibility
        self.model = None
        self.tokenizer = None
        self.id_to_intent = None
        self.intent_to_command = self.nlp_processor.get_intent_to_command_mapping()
        
        # Legacy entity patterns ƒë√£ ƒë∆∞·ª£c chuy·ªÉn v√†o entity_extractor.py
        # v√† ƒë∆∞·ª£c c·∫£i thi·ªán trong communication_optimizer.py
        
        self.reasoning_engine = ReasoningEngine()
        print("Reasoning Engine initialized")
    
    def _extract_entities_simple(self, text: str) -> dict:
        """Extract entities ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£"""
        entities = {}
        text_lower = text.lower()
        
        # RECEIVER - L·∫•y ng∆∞·ªùi nh·∫≠n
        receiver_match = re.search(r"cho\s+((?:ba|b·ªë|m·∫π|anh|ch·ªã|em|c√¥|ch√∫|b√°c|√¥ng|b√†)\s*[\w\s]*?)(?:\s+(?:r·∫±ng|l√†|n√≥i|nh·∫Øn|g·ª≠i|l√∫c|t·∫°i|·ªü|v√†o|ng√†y|gi·ªù|$))", text_lower)
        if receiver_match:
            entities["RECEIVER"] = receiver_match.group(1).strip()
        
        # TIME - L·∫•y th·ªùi gian
        time_match = re.search(r"(s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m|\d{1,2}h)", text_lower)
        if time_match:
            entities["TIME"] = time_match.group(1).strip()
        
        # LOCATION - L·∫•y ƒë·ªãa ƒëi·ªÉm
        location_match = re.search(r"t·∫°i\s+((?:b·ªánh vi·ªán|tr∆∞·ªùng|c√¥ng vi√™n|nh√†|c√¥ng ty|vƒÉn ph√≤ng|ph√≤ng)\s*[\w\s]*?)(?:\s+(?:l√∫c|gi·ªù|v√†o|ng√†y|$))", text_lower)
        if location_match:
            entities["LOCATION"] = location_match.group(1).strip()
        
        # PLATFORM - M·∫∑c ƒë·ªãnh SMS
        platform_match = re.search(r"qua\s+(zalo|facebook|messenger|telegram|instagram|tiktok)", text_lower)
        if platform_match:
            entities["PLATFORM"] = platform_match.group(1).lower()
        else:
            entities["PLATFORM"] = "sms"
        
        # MESSAGE - L·∫•y n·ªôi dung tin nh·∫Øn ƒë·∫ßy ƒë·ªß (bao g·ªìm c·∫£ ph·∫ßn sau d·∫•u ph·∫©y)
        message_patterns = [
            r"l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$)",  # "l√†" + to√†n b·ªô n·ªôi dung ƒë·∫øn cu·ªëi c√¢u
            r"r·∫±ng\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$)",  # "r·∫±ng" + to√†n b·ªô n·ªôi dung ƒë·∫øn cu·ªëi c√¢u
            r"n√≥i\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$)"   # "n√≥i" + to√†n b·ªô n·ªôi dung ƒë·∫øn cu·ªëi c√¢u
        ]
        
        for pattern in message_patterns:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match and match.group(1):
                message = match.group(1).strip()
                if message and len(message) > 5:
                    entities["MESSAGE"] = message
                    break
        
        return entities
    
    def _predict_intent_simple(self, text: str) -> tuple:
        """Predict intent ƒë∆°n gi·∫£n v√† nhanh"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["nh·∫Øn", "tin", "g·ª≠i", "message", "sms"]):
            return "send-mess", 0.9
        elif any(word in text_lower for word in ["g·ªçi", "ƒëi·ªán", "phone", "call"]):
            return "call", 0.9
        elif any(word in text_lower for word in ["b√°o th·ª©c", "nh·∫Øc", "alarm", "reminder"]):
            return "set-alarm", 0.9
        else:
            return "unknown", 0.0
    
    def load_model(self):
        """Load trained model v·ªõi h·ªó tr·ª£ large model"""
        try:
                        
            # Load config
            from src.training.configs.config import ModelConfig
            model_config = ModelConfig()
            
            model_dir = f"models/phobert_{model_config.model_size}_intent_model"
            
            best_model_path = None
            if os.path.exists(model_dir):
                best_models = []
                for filename in os.listdir(model_dir):
                    if filename.endswith('.pth') and 'best' in filename:
                        file_path = f"{model_dir}/{filename}"
                        file_time = os.path.getmtime(file_path)
                        best_models.append((file_path, file_time))
                
                if best_models:
                    best_models.sort(key=lambda x: x[1], reverse=True)
                    best_model_path = best_models[0][0]
                    print(f"üéØ Found latest best model: {os.path.basename(best_model_path)}")
            
            if not best_model_path:
                model_path = f"{model_dir}/model.pth"
                if not os.path.exists(model_path):
                    model_path = "models/best_simple_intent_model.pth"
                    if not os.path.exists(model_path):
                        print(f"No model found, using reasoning engine only")
                        self.model = None
                        self.tokenizer = None
                        self.id_to_intent = None
                        return True
            else:
                model_path = best_model_path
            
            print(f"Loading model from: {model_path}")
            
            checkpoint = torch.load(model_path, map_location=self.device)
            
            if 'intent_to_id' not in checkpoint or 'id_to_intent' not in checkpoint:
                print("Model kh√¥ng c√≥ metadata, s·ª≠ d·ª•ng reasoning engine only")
                self.model = None
                self.tokenizer = None
                self.id_to_intent = None
                return True
            
            # T·∫°o model v·ªõi config
            self.model = SimpleIntentModel(model_config.model_name, len(checkpoint['intent_to_id']), model_config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            self.model.to(self.device)
            
            # Enable mixed precision n·∫øu c√≥ GPU v√† config cho ph√©p
            if self.device.type == "cuda" and model_config.use_fp16:
                self.model = self.model.half()  # Convert to FP16
                print("üîß Enabled FP16 for GPU inference")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)
            
            self.id_to_intent = checkpoint['id_to_intent']
            
            model_info = {
                'model_size': checkpoint.get('model_size', 'unknown'),
                'total_parameters': checkpoint.get('total_parameters', 0),
                'trainable_parameters': checkpoint.get('trainable_parameters', 0),
                'epoch': checkpoint.get('epoch', 'unknown'),
                'is_best': checkpoint.get('is_best', False),
                'validation_accuracy': checkpoint.get('validation_accuracy', 'unknown')
            }
            
            print(f"Model loaded successfully from {model_path}")
            print(f"Number of intents: {len(self.id_to_intent)}")
            print(f"Model info: {model_info}")
            print(f"Available intents: {list(self.id_to_intent.values())}")
            
            return True
            
        except Exception as e:
            print(f"Error loading model: {e}")
            import traceback
            traceback.print_exc()
            self.model = None
            self.tokenizer = None
            self.id_to_intent = None
            return True
    
    def extract_entities(self, text: str) -> Dict[str, str]:
        """Extract entities using NLPProcessor"""
        return self.nlp_processor.entity_extractor.extract_all_entities(text)
    
    def generate_value(self, intent: str, entities: Dict[str, str], original_text: str) -> str:
        """Generate value using NLPProcessor"""
        return self.nlp_processor.value_generator.generate_value(intent, entities, original_text)
    
    def predict_intent(self, text: str, confidence_threshold: float = 0.3) -> Dict:
        """Predict intent using NLPProcessor"""
        return self.nlp_processor.intent_predictor.predict_intent(text, confidence_threshold)
    
    async def process_text(self, text: str, confidence_threshold: float = 0.3) -> IntentResponse:
        """Process text using NLPProcessor"""
        result = self.nlp_processor.process_text(text, confidence_threshold)
        
        return IntentResponse(
            input_text=result["input_text"],
            intent=result["intent"],
            confidence=result["confidence"],
            command=result["command"],
            entities=result["entities"],
            value=result["value"],
            processing_time=result["processing_time"],
            timestamp=result["timestamp"]
        )
    
    async def predict_with_reasoning(self, text: str) -> Dict[str, Any]:
        """Predict with reasoning using NLPProcessor"""
        return await self.nlp_processor.process_with_reasoning(text)
    
    def load_model(self) -> bool:
        """Load model using NLPProcessor"""
        # Try to find model file - Updated paths to match actual model location
        model_paths = [
            "models/phobert_large_intent_model/model_epoch_10_best.pth",
            "models/phobert_large_intent_model/model_epoch_3_best.pth", 
            "models/phobert_large_intent_model/model_epoch_1_best.pth",
            "models/phobert_large_intent_model/model_best.pth"
        ]
        
        for model_path in model_paths:
            if os.path.exists(model_path):
                print(f"Found model at: {model_path}")
                success = self.nlp_processor.load_model(model_path)
                if success:
                    # Update legacy attributes
                    self.model = self.nlp_processor.intent_predictor.model
                    self.tokenizer = self.nlp_processor.intent_predictor.tokenizer
                    self.id_to_intent = self.nlp_processor.intent_predictor.id_to_intent
                    print(f"Successfully loaded trained model from {model_path}")
                    return True
                else:
                    print(f"Failed to load model from {model_path}")
        
        print("No trained model found, using fallback methods")
        return True  # Return True to allow fallback methods
    
    # Legacy methods for backward compatibility
    def _extract_entities_simple(self, text: str) -> dict:
        """Legacy method - redirect to new extractor"""
        return self.extract_entities(text)
    
    def _predict_intent_simple(self, text: str) -> tuple:
        """Legacy method - redirect to new predictor"""
        result = self.predict_intent(text)
        return result["intent"], result["confidence"]
    
    
    
    # Old generate_value method removed - now using NLPProcessor
    def _old_generate_value(self, intent: str, entities: Dict[str, str], original_text: str) -> str:
        if intent == "unknown" or intent == "error":
            return "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh"
        
        if intent in ["call", "call", "make-video-call"]:
            receiver = entities.get("RECEIVER", "")
            if not receiver:
                potential_receivers = re.findall(r"(?:g·ªçi|g·ªçi cho|g·ªçi ƒëi·ªán cho|nh·∫Øn tin cho|g·ª≠i cho)\s+(\w+(?:\s+\w+){0,2})", original_text, re.IGNORECASE)
                if potential_receivers:
                    receiver = potential_receivers[0]
                else:
                    receiver = "ng∆∞·ªùi nh·∫≠n"
            
            if intent == "make-video-call":
                return f"G·ªçi video cho {receiver}"
            else:
                return f"G·ªçi ƒëi·ªán cho {receiver}"
        
        elif intent in ["send-mess", "send-mess"]:
            message = entities.get("MESSAGE", "")
            receiver = entities.get("RECEIVER", "")
            
            if "ki·ªÉm tra" in original_text.lower() and "t·ª´" in original_text.lower():
                match = re.search(r"t·ª´\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if match:
                    from_person = match.group(1).strip()
                    return f"t·ª´ {from_person}"
            
            if message:
                if "r·∫±ng l√†" in message:
                    content = message.split("r·∫±ng l√†", 1)[-1].strip()
                    if content.startswith("l√† "):
                        content = content[3:].strip()
                    return content
                elif message.startswith("l√† "):
                    content = message[3:].strip()
                    return content
                elif "r·∫±ng" in message:
                    content = message.split("r·∫±ng", 1)[-1].strip()
                    if content.startswith("l√† "):
                        content = content[3:].strip()
                    return content
                elif " l√† " in message:
                    content = message.split(" l√† ", 1)[-1].strip()
                    return content
                else:
                    return message
            
            else:
                patterns = [
                    r"r·∫±ng\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"r·∫±ng\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫Øn|g·ª≠i|nh·∫Øn tin|g·ª≠i tin nh·∫Øn)(?:\s+cho\s+\w+)?(?:\s+qua\s+\w+)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        content = match.group(1).strip()
                        if content.startswith("l√† "):
                            content = content[3:].strip()
                        return content
                
                if receiver:
                    return f"Tin nh·∫Øn cho {receiver}"
                else:
                    return "N·ªôi dung tin nh·∫Øn"
        
        elif intent in ["set-alarm", "set-reminder"]:
            time_info = entities.get("TIME", "")
            
            if not time_info:
                time_patterns = [
                    r"(\d{1,2})\s*(?:gi·ªù|h|:)\s*(\d{1,2})?\s*(?:ph√∫t)?",
                    r"(\d{1,2})\s*(?:gi·ªù|h)\s*(?:r∆∞·ª°i|bu·ªïi|s√°ng|tr∆∞a|chi·ªÅu|t·ªëi)",
                    r"(\d{1,2})\s*(?:gi·ªù|h)"
                ]
                
                for pattern in time_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        if match.group(2):  # Hour and minute
                            time_info = f"{match.group(1)}:{match.group(2)}"
                        else:  # Just hour
                            time_info = f"{match.group(1)}:00"
                        break
            
            if time_info:
                period_match = re.search(r"(s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m)", original_text, re.IGNORECASE)
                if period_match and period_match.group(1) not in time_info:
                    time_info = f"{time_info} {period_match.group(1)}"
            
            if intent == "set-alarm":
                description = "B√°o th·ª©c"
                description_match = re.search(r"(?:b√°o th·ª©c|alarm)(?:\s+ƒë·ªÉ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if description_match:
                    description = description_match.group(1).strip()
                
                return f"{time_info} - {description}" if time_info else "Th·ªùi gian b√°o th·ª©c"
            else:  # set-reminder
                description = "Nh·∫Øc nh·ªü"
                
                medicine_patterns = [
                    r"u·ªëng\s+(\d+\s+)?(?:vi√™n\s+)?(?:thu·ªëc\s+)?(?:ti·ªÉu\s+ƒë∆∞·ªùng|huy·∫øt\s+√°p|tim|vitamin|s·∫Øt|c·∫£m|ƒëau\s+ƒë·∫ßu|kh√°ng\s+sinh)",
                    r"(?:thu·ªëc\s+)?(?:ti·ªÉu\s+ƒë∆∞·ªùng|huy·∫øt\s+√°p|tim|vitamin|s·∫Øt|c·∫£m|ƒëau\s+ƒë·∫ßu|kh√°ng\s+sinh)",
                    r"u·ªëng\s+(.+?)(?:\s+(?:l√∫c|v√†o|sau|tr∆∞·ªõc|nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫Øc nh·ªü|nh·∫Øc|reminder)(?:\s+v·ªÅ|v·ªÅ|v·ªÅ vi·ªác|vi·ªác)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in medicine_patterns:
                    description_match = re.search(pattern, original_text, re.IGNORECASE)
                    if description_match:
                        if description_match.groups():
                            description = description_match.group(1).strip()
                        else:
                            description = description_match.group(0).strip()
                        break
                
                return f"{time_info} - {description}" if time_info else description
        
        elif intent == "check-weather":
            location = entities.get("LOCATION", "")
            time = entities.get("TIME", "")
            
            if not location:
                location_match = re.search(r"(?:th·ªùi ti·∫øt|nhi·ªát ƒë·ªô|m∆∞a|n·∫Øng)(?:\s+·ªü|t·∫°i|c·ªßa)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if location_match:
                    location = location_match.group(1).strip()
                else:
                    location = "khu v·ª±c hi·ªán t·∫°i"
            
            if time:
                return f"{location} ({time})"
            else:
                return location
        
        elif intent == "check-device-status":
            device = entities.get("DEVICE", "thi·∫øt b·ªã")
            return f"Ki·ªÉm tra tr·∫°ng th√°i {device}"
        
        elif intent == "check-health-status":
            health_aspect = entities.get("HEALTH", "s·ª©c kh·ªèe")
            return f"Ki·ªÉm tra {health_aspect}"
        
        elif intent == "check-messages":
            platform = entities.get("PLATFORM", "")
            receiver = entities.get("RECEIVER", "")
            
            if "t·ª´" in original_text.lower():
                match = re.search(r"t·ª´\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if match:
                    from_person = match.group(1).strip()
                    return f"t·ª´ {from_person}"
            
            if receiver and receiver != "tr√™n":  # "tr√™n" kh√¥ng ph·∫£i ng∆∞·ªùi g·ª≠i
                return f"t·ª´ {receiver}"
            elif platform:
                return f"Ki·ªÉm tra {platform}"
            else:
                return "Ki·ªÉm tra tin nh·∫Øn"
        
        elif intent in ["play-media", "play-audio", "play-content"]:
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            platform = entities.get("PLATFORM", "")
            
            if not content and not query:
                content_patterns = [
                    r"(?:ph√°t|m·ªü|b·∫≠t|nghe|xem)(?:\s+b√†i|nh·∫°c|phim|video|clip)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:b√†i h√°t|b√†i|ca kh√∫c|nh·∫°c|phim|video|clip)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m ki·∫øm|t√¨m|search)\s+(?:tr√™n|qua|b·∫±ng|d√πng)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:danh s√°ch|list|playlist)\s+(?:nh·∫°c|music|video|clip|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|music|video|clip|phim)\s+(?:m·ªõi nh·∫•t|hot|trending|ph·ªï bi·∫øn)\s+(?:c·ªßa|do|b·ªüi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in content_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        content = match.group(1).strip()
                        break
            
            final_content = content if content else query
            
            if final_content:
                if platform:
                    return f"{final_content} tr√™n {platform}"
                else:
                    return final_content
            else:
                return "N·ªôi dung ph√°t"
        
        elif intent in ["read-news", "read-content"]:
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            
            if not content and not query:
                content_match = re.search(r"(?:ƒë·ªçc|ƒë·ªçc tin|ƒë·ªçc b√°o|ƒë·ªçc tin t·ª©c)(?:\s+v·ªÅ|v·ªÅ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if content_match:
                    content = content_match.group(1).strip()
            
            if intent == "read-news":
                if content:
                    return f"Tin t·ª©c v·ªÅ {content}"
                elif query:
                    return f"Tin t·ª©c v·ªÅ {query}"
                else:
                    return "Tin t·ª©c"
            else:  # read-content
                if content:
                    return f"ƒê·ªçc: {content}"
                elif query:
                    return f"ƒê·ªçc v·ªÅ: {query}"
                else:
                    return "N·ªôi dung ƒë·ªçc"
        
        elif intent == "view-content":
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            
            if content:
                return f"Xem: {content}"
            elif query:
                return f"Xem v·ªÅ: {query}"
            else:
                return "N·ªôi dung xem"
        
        elif intent == "open-app":
            app = entities.get("APP", "")
            
            if not app:
                app_match = re.search(r"(?:m·ªü|v√†o|kh·ªüi ƒë·ªông|ch·∫°y|s·ª≠ d·ª•ng|d√πng)(?:\s+·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if app_match:
                    app = app_match.group(1).strip()
                else:
                    app = "·ª©ng d·ª•ng"
            
            return app
        
        elif intent == "open-app-action":
            app = entities.get("APP", "·ª©ng d·ª•ng")
            action = entities.get("ACTION", "h√†nh ƒë·ªông")
            return f"{action} trong {app}"
        
        elif intent in ["search-content", "search-internet"]:
            query = entities.get("QUERY", "")
            platform = entities.get("PLATFORM", "")
            
            if not query:
                query_patterns = [
                    r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|ki·∫øm|t√¨m hi·ªÉu)(?:\s+v·ªÅ|v·ªÅ|th√¥ng tin v·ªÅ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|ki·∫øm|t√¨m hi·ªÉu)(?:\s+(?:cho t√¥i|cho m√¨nh|cho b√°c|cho c√¥|cho ch√∫|gi√∫p t√¥i|gi√∫p m√¨nh|gi√∫p b√°c|gi√∫p c√¥|gi√∫p ch√∫))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m ki·∫øm|t√¨m|search)\s+(?:tr√™n|qua|b·∫±ng|d√πng)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:danh s√°ch|list|playlist)\s+(?:nh·∫°c|music|video|clip|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|music|video|clip|phim)\s+(?:m·ªõi nh·∫•t|hot|trending|ph·ªï bi·∫øn)\s+(?:c·ªßa|do|b·ªüi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in query_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        query = match.group(1).strip()
                        break
            
            if query:
                if platform:
                    return f"{query} tr√™n {platform}"
                else:
                    return query
            else:
                return "T·ª´ kh√≥a t√¨m ki·∫øm"
        
        elif intent == "browse-social-media":
            platform = entities.get("PLATFORM", "")
            
            if not platform:
                platform_match = re.search(r"(?:l∆∞·ªõt|duy·ªát|xem|v√†o|m·ªü)(?:\s+(?:facebook|fb|zalo|instagram|tiktok|youtube|twitter))(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if platform_match:
                    platform = platform_match.group(1).strip()
                else:
                    platform = "m·∫°ng x√£ h·ªôi"
            
            return f"Duy·ªát {platform}"
        
        elif intent == "control-device":
            device = entities.get("DEVICE", "thi·∫øt b·ªã")
            action = entities.get("ACTION", "")
            
            if not action:
                action_match = re.search(r"(?:b·∫≠t|t·∫Øt|m·ªü|ƒë√≥ng|kh√≥a|m·ªü kh√≥a|ƒëi·ªÅu ch·ªânh|tƒÉng|gi·∫£m|thay ƒë·ªïi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if action_match:
                    action = action_match.group(0).strip()  # Use full match as the action
                else:
                    action = "ƒëi·ªÅu khi·ªÉn"
            
            return f"{action} {device}"
        
        elif intent == "adjust-settings":
            setting = entities.get("SETTING", "")
            
            if not setting:
                setting_match = re.search(r"(?:c√†i ƒë·∫∑t|thi·∫øt l·∫≠p|ƒëi·ªÅu ch·ªânh|thay ƒë·ªïi|ch·ªânh|s·ª≠a)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if setting_match:
                    setting = setting_match.group(1).strip()
                else:
                    setting = "c√†i ƒë·∫∑t"
            
            return f"ƒêi·ªÅu ch·ªânh {setting}"
        
        elif intent == "app-tutorial":
            app = entities.get("APP", "")
            
            if not app:
                app_match = re.search(r"(?:h∆∞·ªõng d·∫´n|ch·ªâ d·∫´n|ch·ªâ|d·∫°y|b√†y)(?:\s+(?:s·ª≠ d·ª•ng|d√πng|c√°ch))?(?:\s+(?:·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if app_match:
                    app = app_match.group(1).strip()
                else:
                    app = "·ª©ng d·ª•ng"
            
            return f"H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng {app}"
        
        elif intent == "navigation-help":
            destination = entities.get("LOCATION", "")
            
            if not destination:
                destination_match = re.search(r"(?:ƒë∆∞·ªùng|ƒë∆∞·ªùng ƒëi|ch·ªâ ƒë∆∞·ªùng|ch·ªâ|ƒëi|t·ªõi|ƒë·∫øn|v·ªÅ)(?:\s+(?:t·ªõi|ƒë·∫øn|v·ªÅ))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if destination_match:
                    destination = destination_match.group(1).strip()
                else:
                    destination = "ƒë√≠ch ƒë·∫øn"
            
            return f"ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn {destination}"
        
        elif intent == "provide-instructions":
            topic = entities.get("TOPIC", "")
            
            if not topic:
                topic_match = re.search(r"(?:h∆∞·ªõng d·∫´n|ch·ªâ d·∫´n|ch·ªâ|d·∫°y|b√†y)(?:\s+(?:v·ªÅ|c√°ch))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if topic_match:
                    topic = topic_match.group(1).strip()
                else:
                    topic = "ch·ªß ƒë·ªÅ"
            
            return f"H∆∞·ªõng d·∫´n v·ªÅ {topic}"
        
        elif intent == "general-conversation":
            if "xin ch√†o" in original_text.lower() or "hello" in original_text.lower() or "hi" in original_text.lower():
                return "Ch√†o h·ªèi"
            elif "t·∫°m bi·ªát" in original_text.lower() or "bye" in original_text.lower():
                return "T·∫°m bi·ªát"
            elif "c·∫£m ∆°n" in original_text.lower() or "thanks" in original_text.lower() or "thank" in original_text.lower():
                return "C·∫£m ∆°n"
            elif "xin l·ªói" in original_text.lower() or "sorry" in original_text.lower():
                return "Xin l·ªói"
            elif "kh·ªèe kh√¥ng" in original_text.lower() or "th·∫ø n√†o" in original_text.lower():
                return "H·ªèi thƒÉm"
            else:
                return "Tr√≤ chuy·ªán th√¥ng th∆∞·ªùng"
        
        else:
            if "t√¨m" in original_text.lower() or "t√¨m ki·∫øm" in original_text.lower() or "search" in original_text.lower():
                search_patterns = [
                    r"(?:t√¨m|t√¨m ki·∫øm|search)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+\w+\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in search_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            elif "ph√°t" in original_text.lower() or "nghe" in original_text.lower() or "xem" in original_text.lower():
                media_patterns = [
                    r"(?:ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|video|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in media_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            elif "m·ªü" in original_text.lower() or "v√†o" in original_text.lower():
                app_patterns = [
                    r"(?:m·ªü|v√†o)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:·ª©ng d·ª•ng|app)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in app_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            return f"Th·ª±c hi·ªán h√†nh ƒë·ªông: {intent}"
    
    # Old predict_intent method removed - now using NLPProcessor
    def _old_predict_intent(self, text: str, confidence_threshold: float = 0.3) -> Dict:
        """Old predict intent method - replaced by NLPProcessor"""
        try:
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=model_config.max_length,
                return_tensors="pt"
            )
            
            input_ids = encoding["input_ids"].to(self.device)
            attention_mask = encoding["attention_mask"].to(self.device)
            
            if self.model.dtype == torch.float16:
                input_ids = input_ids.half()
                attention_mask = attention_mask.half()
            
            with torch.no_grad():
                logits = self.model(input_ids, attention_mask)
                probabilities = torch.softmax(logits, dim=1)
                predicted = torch.argmax(logits, dim=1)
                confidence = probabilities.max().item()
                intent = self.id_to_intent[predicted.item()]
            
            if confidence < confidence_threshold:
                intent = "unknown"
                confidence = 0.0
            
            return {
                "intent": intent,
                "confidence": confidence,
                "probabilities": probabilities.cpu().numpy().tolist()[0]
            }
            
        except Exception as e:
            print(f"Error predicting intent: {e}")
            return {
                "intent": "error",
                "confidence": 0.0,
                "probabilities": []
            }
    
    # Old predict_with_reasoning method removed - now using NLPProcessor
    async def _old_predict_with_reasoning(self, text: str) -> Dict[str, Any]:
        """Old predict with reasoning method - replaced by NLPProcessor"""
        try:
            print(f"REASONING PREDICTION: '{text}'")
            start_time = datetime.now()
            
            try:
                model_result = None
                model_confidence = 0.0
                model_intent = "unknown"
                
                if self.model and self.tokenizer:
                    model_result = self.predict_intent(text)
                    model_confidence = model_result.get("confidence", 0.0)
                    model_intent = model_result.get("intent", "unknown")
                    print(f"MODEL PREDICTION: {model_intent} (confidence: {model_confidence:.3f})")
            except Exception as e:
                print(f"Model prediction error: {str(e)}")
                model_result = {"intent": "unknown", "confidence": 0.0}
                model_confidence = 0.0
                model_intent = "unknown"
            
            if model_confidence < 0.6 or model_intent == "unknown":
                                
                text_lower = text.lower()
                message_keywords = ["nh·∫Øn tin", "g·ª≠i tin", "so·∫°n tin", "text", "sms", "message", "g·ª≠i", "nh·∫Øn"]
                has_message_keyword = any(keyword in text_lower for keyword in message_keywords)
                
                try:
                    reasoning_result = self.reasoning_engine.reasoning_predict(text)
                    reasoning_intent = reasoning_result.get("intent", "unknown")
                    reasoning_confidence = reasoning_result.get("confidence", 0.0)
                    print(f"REASONING PREDICTION: {reasoning_intent} (confidence: {reasoning_confidence:.3f})")
                except Exception as e:
                    print(f"Reasoning engine error: {e}")
                    print("Using simple prediction...")
                    reasoning_intent, reasoning_confidence = self._predict_intent_simple(text)
                    reasoning_result = {"intent": reasoning_intent, "confidence": reasoning_confidence}
                    print(f"SIMPLE PREDICTION: {reasoning_intent} (confidence: {reasoning_confidence:.3f})")
                
                call_keywords = ["cu·ªôc g·ªçi", "g·ªçi tho·∫°i", "g·ªçi ƒëi·ªán", "th·ª±c hi·ªán g·ªçi", "th·ª±c hi·ªán cu·ªôc g·ªçi"]
                has_call_keyword = any(keyword in text_lower for keyword in call_keywords)
                
                if has_call_keyword and reasoning_intent != "call":
                    print("üîß Override intent to call due to call keywords")
                    reasoning_intent = "call"
                    reasoning_confidence = max(reasoning_confidence, 0.8)  # Boost confidence
                
                elif has_message_keyword and reasoning_intent != "send-mess":
                    print("üîß Override intent to send-mess due to message keywords")
                    reasoning_intent = "send-mess"
                    reasoning_confidence = max(reasoning_confidence, 0.7)  # Boost confidence
                
                final_intent = reasoning_intent
                final_confidence = reasoning_confidence
                method = "reasoning_engine"
                
                if model_intent != "unknown" and model_confidence >= 0.4:
                    if model_intent == reasoning_intent:
                        final_confidence = max(model_confidence, reasoning_confidence) + 0.1
                        final_confidence = min(final_confidence, 0.99)  # Cap at 0.99
                        method = "model_reasoning_agreement"
                    else:
                        if model_confidence > reasoning_confidence + 0.2:  # Model significantly more confident
                            final_intent = model_intent
                            final_confidence = model_confidence
                            method = "model_override"
            else:
                final_intent = model_intent
                final_confidence = model_confidence
                method = "trained_model"
                reasoning_result = None
            
            try:
                # S·ª≠ d·ª•ng logic extract ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£
                entities = self._extract_entities_simple(text)
                
            except Exception as e:
                print(f"Error extracting entities: {e}")
                entities = {}
            
            command = self.intent_to_command.get(final_intent, "unknown")
            
            try:
                value = self.generate_value(final_intent, entities, text)
            except Exception as e:
                print(f"Error generating value: {e}")
                value = f"Th·ª±c hi·ªán h√†nh ƒë·ªông: {final_intent}"
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "text": text,
                "intent": final_intent,
                "confidence": final_confidence,
                "command": command,
                "entities": entities,
                "value": value,
                "method": method,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat()
            }
            
            if reasoning_result:
                result["reasoning_details"] = {
                    "semantic_similarity": reasoning_result.get("semantic_similarity", {})
                }
            
            return result
                    
        except Exception as e:
            print(f"Error in reasoning prediction: {str(e)}")
            import traceback
            return {
                "text": text,
                "intent": "unknown",
                "confidence": 0.0,
                "command": "unknown",
                "entities": {},
                "value": "",
                "method": "error",
                "error": str(e),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
    
    async def batch_predict_with_reasoning(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Batch predict v·ªõi reasoning engine"""
        results = []
        for text in texts:
            result = await self.predict_with_reasoning(text)
            results.append(result)
        return results
    
    async def analyze_text_semantics(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch semantic c·ªßa text"""
        try:
            embedding = self.reasoning_engine.get_text_embedding(text)
            
            similar_intents = self.reasoning_engine.find_similar_intents(text)
            
            context_features = self.reasoning_engine.extract_context_features(text)
            
            pattern_results = self.reasoning_engine.pattern_matching(text)
            
            keyword_results = self.reasoning_engine.keyword_matching(text)
            
            return {
                "text": text,
                "embedding_shape": embedding.shape,
                "similar_intents": similar_intents,
                "context_features": context_features,
                "pattern_matching": pattern_results,
                "keyword_matching": keyword_results,
                "semantic_analysis": {
                    "has_time_context": context_features["has_time"],
                    "has_person_context": context_features["has_person"],
                    "has_action_context": context_features["has_action"],
                    "has_object_context": context_features["has_object"]
                }
            }
            
        except Exception as e:
            return {
                "text": text,
                "error": str(e)
            }
    
    async def update_knowledge_base(self, new_patterns: Dict[str, List[str]]) -> Dict[str, Any]:
        """C·∫≠p nh·∫≠t knowledge base c·ªßa reasoning engine"""
        try:
            self.reasoning_engine.update_knowledge_base(new_patterns)
            return {
                "status": "success",
                "message": "Knowledge base updated successfully",
                "updated_patterns": new_patterns
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def save_knowledge_base(self, file_path: str = "knowledge_base.json") -> Dict[str, Any]:
        """L∆∞u knowledge base"""
        try:
            self.reasoning_engine.save_knowledge_base(file_path)
            return {
                "status": "success",
                "message": f"Knowledge base saved to {file_path}",
                "file_path": file_path
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def load_knowledge_base(self, file_path: str = "knowledge_base.json") -> Dict[str, Any]:
        """Load knowledge base"""
        try:
            self.reasoning_engine.load_knowledge_base(file_path)
            return {
                "status": "success",
                "message": f"Knowledge base loaded from {file_path}",
                "file_path": file_path
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

    # Old process_text method removed - now using NLPProcessor
    async def _old_process_text(self, text: str, confidence_threshold: float = 0.3) -> IntentResponse:
        """Old process text method - replaced by NLPProcessor"""
        start_time = datetime.now()
        
        intent_result = self.predict_intent(text, confidence_threshold)
        
        entities = self.extract_entities(text)
        
        command = self.intent_to_command.get(intent_result["intent"], "unknown")
        
        value = self.generate_value(intent_result["intent"], entities, text)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return IntentResponse(
            input_text=text,
            intent=intent_result["intent"],
            confidence=intent_result["confidence"],
            command=command,
            entities=entities,
            value=value,
            processing_time=processing_time,
            timestamp=datetime.now().isoformat()
        )

api = PhoBERT_SAM_API()

app = FastAPI(
    title="PhoBERT_SAM API",
    description="API cho Intent Recognition v√† Entity Extraction cho ng∆∞·ªùi cao tu·ªïi",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins (c√≥ th·ªÉ thay ƒë·ªïi th√†nh specific domains)
    allow_credentials=True,
    allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ HTTP methods
    allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers
)

@app.on_event("startup")
async def startup_event():
    """Kh·ªüi t·∫°o model khi server start"""
    if not api.load_model():
        raise Exception("Kh√¥ng th·ªÉ load model!")

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "PhoBERT_SAM API",
        "version": "1.0.0",
        "status": "running",
        "available_intents": list(api.intent_to_command.keys()) if api.id_to_intent else []
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": api.model is not None,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict")
async def predict_intent(request: IntentRequest):
    """Predict intent v√† extract entities - Simplified response"""
    try:
        if not api.model:
            reasoning_result = await api.predict_with_reasoning(request.text)
            
            return {
                "text": request.text,
                "intent": reasoning_result["intent"],
                "confidence": reasoning_result["confidence"],
                "command": reasoning_result["command"],
                "entities": reasoning_result["entities"],
                "value": reasoning_result["value"],
                "method": reasoning_result.get("method", "reasoning_engine"),
                "processing_time": reasoning_result["processing_time"],
                "timestamp": reasoning_result["timestamp"],
                "reasoning_details": reasoning_result.get("reasoning_details", {})
            }
        
        result = await api.process_text(request.text, request.confidence_threshold)
        
        return {
            "text": result.input_text,
            "intent": result.intent,
            "confidence": result.confidence,
            "command": result.command,
            "entities": result.entities,
            "value": result.value,
            "method": "trained_model",
            "processing_time": result.processing_time,
            "timestamp": result.timestamp
        }
        
    except Exception as e:
        print(f"Error in predict endpoint: {str(e)}")
        
        # Return simplified error response
        return {
            "text": request.text,
            "intent": "unknown",
            "confidence": 0.0,
            "command": "unknown",
            "entities": {},
            "value": "",
            "method": "error",
            "processing_time": 0.0,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/intents")
async def get_intents():
    """L·∫•y danh s√°ch intents c√≥ s·∫µn"""
    if not api.id_to_intent:
        raise HTTPException(status_code=500, detail="Model ch∆∞a ƒë∆∞·ª£c load")
    
    return {
        "intents": list(api.id_to_intent.values()),
        "intent_to_command": api.intent_to_command
    }

@app.get("/entities")
async def get_entity_patterns():
    """L·∫•y patterns cho entity extraction"""
    return {
        "entity_patterns": api.entity_patterns
    }

@app.post("/batch_predict")
async def batch_predict(texts: List[str], confidence_threshold: float = 0.3):
    """Predict nhi·ªÅu texts c√πng l√∫c"""
    try:
        if not api.model:
            raise HTTPException(status_code=500, detail="Model ch∆∞a ƒë∆∞·ª£c load")
        
        results = []
        for text in texts:
            result = await api.process_text(text, confidence_threshold)
            results.append(result.dict())
        
        return {
            "results": results,
            "total_processed": len(texts)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω batch: {str(e)}")

@app.post("/predict-with-reasoning")
async def predict_with_reasoning(request: Dict[str, Any]):
    """Predict intent v·ªõi reasoning engine"""
    text = request.get("text", "")
    if not text:
        return {"error": "Text is required"}
    
    result = await api.predict_with_reasoning(text)
    return result

@app.post("/batch-predict-with-reasoning")
async def batch_predict_with_reasoning(request: Dict[str, Any]):
    """Batch predict v·ªõi reasoning engine"""
    texts = request.get("texts", [])
    if not texts:
        return {"error": "Texts list is required"}
    
    results = await api.batch_predict_with_reasoning(texts)
    return {"results": results}

@app.post("/analyze-semantics")
async def analyze_semantics(request: Dict[str, Any]):
    """Ph√¢n t√≠ch semantic c·ªßa text"""
    text = request.get("text", "")
    if not text:
        return {"error": "Text is required"}
    
    result = await api.analyze_text_semantics(text)
    return result

@app.post("/update-knowledge-base")
async def update_knowledge_base(request: Dict[str, Any]):
    """C·∫≠p nh·∫≠t knowledge base"""
    new_patterns = request.get("patterns", {})
    if not new_patterns:
        return {"error": "Patterns are required"}
    
    result = await api.update_knowledge_base(new_patterns)
    return result

@app.post("/save-knowledge-base")
async def save_knowledge_base(request: Dict[str, Any]):
    """L∆∞u knowledge base"""
    file_path = request.get("file_path", "knowledge_base.json")
    result = await api.save_knowledge_base(file_path)
    return result

@app.post("/load-knowledge-base")
async def load_knowledge_base(request: Dict[str, Any]):
    """Load knowledge base"""
    file_path = request.get("file_path", "knowledge_base.json")
    result = await api.load_knowledge_base(file_path)
    return result

if __name__ == "__main__":
    print("üöÄ Starting PhoBERT_SAM API Server...")
    print("=" * 50)
    print("üìñ API Documentation: http://localhost:8000/docs")
    print("üéØ Predict Endpoint: POST http://localhost:8000/predict")
    print("=" * 50)
    
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
