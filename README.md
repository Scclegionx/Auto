# PhoBERT_SAM - Há»‡ Thá»‘ng NLP Cho NgÆ°á»i Cao Tuá»•i


- **Intent Recognition**: Nháº­n diá»‡n Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng (gá»i Ä‘iá»‡n, gá»­i tin nháº¯n, Ä‘áº·t bÃ¡o thá»©c, v.v.)
- **Entity Extraction**: TrÃ­ch xuáº¥t thÃ´ng tin quan trá»ng (ngÆ°á»i nháº­n, thá»i gian, tin nháº¯n, Ä‘á»‹a Ä‘iá»ƒm)
- **Command Mapping**: Chuyá»ƒn Ä‘á»•i intent thÃ nh command thá»±c thi
- **Value Generation**: Táº¡o mÃ´ táº£ hÃ nh Ä‘á»™ng cá»¥ thá»ƒ
- **Web Interface**: Giao diá»‡n web Ä‘áº¹p máº¯t Ä‘á»ƒ test API
- **RESTful API**: API hoÃ n chá»‰nh vá»›i FastAPI



### 1. CÃ i Äáº·t Dependencies
```bash
pip install -r requirements.txt
```

### 2. Kiá»ƒm Tra CÃ i Äáº·t
```bash
python -c "import torch; print('PyTorch:', torch.__version__)"
python -c "import transformers; print('Transformers:', transformers.__version__)"
```

## ğŸ¯ Sá»­ Dá»¥ng Nhanh

### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng API Server
```bash
python api_server.py
```
Server sáº½ cháº¡y táº¡i: http://localhost:8000

### BÆ°á»›c 2: Má»Ÿ Giao Diá»‡n Web
- Má»Ÿ file `web_interface.html` trong trÃ¬nh duyá»‡t
- Hoáº·c truy cáº­p: http://localhost:8000/docs (API documentation)


## Training Model 

### 1. Chuáº©n Bá»‹ Dataset
Há»‡ thá»‘ng Ä‘Ã£ cÃ³ sáºµn dataset:
- `elderly_command_dataset_reduced.json`: 1,951 samples
- `nlp_command_dataset.json`: 3,062 samples

### 2. Data Augmentation (TÃ¹y chá»n)
```bash
python data_augmentation.py
```

### 3. Training Model
```bash
python simple_train.py
```
## Cáº¥u TrÃºc Dá»± Ãn

```
Auto_NLP/
â”œâ”€â”€ api_server.py              # FastAPI server chÃ­nh
â”œâ”€â”€ web_interface.html         # Giao diá»‡n web
â”œâ”€â”€ simple_train.py            # Script training Ä‘Æ¡n giáº£n
â”œâ”€â”€ config.py                  # Cáº¥u hÃ¬nh há»‡ thá»‘ng
â”œâ”€â”€ data_augmentation.py       # TÄƒng cÆ°á»ng dá»¯ liá»‡u
â”œâ”€â”€ utils.py                   # Tiá»‡n Ã­ch
â”œâ”€â”€ main.py                    # Training pipeline Ä‘áº§y Ä‘á»§
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ TRAINING_GUIDE.md          # HÆ°á»›ng dáº«n training chi tiáº¿t
â”œâ”€â”€ models/                    # ThÆ° má»¥c chá»©a model
â”‚   â”œâ”€â”€ best_simple_intent_model.pth  # Model Ä‘Ã£ train
â”‚   â”œâ”€â”€ intent_model.py        # Model architecture
â”‚   â”œâ”€â”€ entity_model.py        # Entity extraction model
â”‚   â”œâ”€â”€ command_model.py       # Command processing model
â”‚   â””â”€â”€ unified_model.py       # Unified model
â”œâ”€â”€ data/                      # Data processing
â”‚   â””â”€â”€ data_processor.py      # Data processing utilities
â”œâ”€â”€ training/                  # Training utilities
â”‚   â””â”€â”€ trainer.py             # Training class
â”œâ”€â”€ logs/                      # Training logs
â””â”€â”€ datasets/                  # Dataset files
    â”œâ”€â”€ elderly_command_dataset_reduced.json
    â””â”€â”€ nlp_command_dataset.json
```

## ğŸ”Œ API Endpoints

### ChÃ­nh
- `GET /` - Root endpoint
- `GET /health` - Health check
- `POST /predict` - Predict intent vÃ  extract entities
- `GET /intents` - Láº¥y danh sÃ¡ch intents
- `GET /entities` - Láº¥y entity patterns
- `POST /batch_predict` - Predict nhiá»u texts

### Request Format
```json
{
  "text": "gá»­i tin nháº¯n cho máº¹ ráº±ng tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "confidence_threshold": 0.3
}
```

### Response Format
```json
{
  "input_text": "gá»­i tin nháº¯n cho máº¹ ráº±ng tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "intent": "send-mess",
  "confidence": 0.643,
  "command": "send_message",
  "entities": {
    "RECEIVER": "máº¹",
    "MESSAGE": "tá»‘i con sáº½ vá» Äƒn cÆ¡m"
  },
  "value": "Gá»­i tin nháº¯n cho máº¹: tá»‘i con sáº½ vá» Äƒn cÆ¡m",
  "processing_time": 0.123,
  "timestamp": "2024-01-15T10:30:00"
}
```

## ğŸ¯ Intent Types

| Intent | Command | MÃ´ Táº£ |
|--------|---------|-------|
| `call` | `make_call` | Gá»i Ä‘iá»‡n thoáº¡i |
| `send-mess` | `send_message` | Gá»­i tin nháº¯n |
| `set-alarm` | `set_alarm` | Äáº·t bÃ¡o thá»©c |
| `set-reminder` | `set_reminder` | Äáº·t nháº¯c nhá»Ÿ |
| `check-weather` | `check_weather` | Kiá»ƒm tra thá»i tiáº¿t |
| `play-media` | `play_media` | PhÃ¡t nháº¡c/phim |
| `read-news` | `read_news` | Äá»c tin tá»©c |
| `check-health-status` | `check_health` | Kiá»ƒm tra sá»©c khá»e |
| `general-conversation` | `chat` | TrÃ² chuyá»‡n thÃ´ng thÆ°á»ng |


## Cáº¥u HÃ¬nh

### Model Config (`config.py`)
```python
model_size: "base"              # "base" hoáº·c "large"
max_length: 128                 # Äá»™ dÃ i tá»‘i Ä‘a input
batch_size: 8                   # Batch size cho training
learning_rate: 1e-5             # Learning rate
num_epochs: 15                  # Sá»‘ epochs training
```

### Training Config
```python
device: "cpu"                   # "cpu" hoáº·c "cuda"
confidence_threshold: 0.3-0.5       # NgÆ°á»¡ng tin cáº­y khuyáº¿n nghá»‹
```

## Troubleshooting


## ğŸ“Š Performance

- **Model Size**: PhoBERT-base (500MB) # tÆ°Æ¡ng lai sáº½ sá»­ dá»¥ng PhoBERT-large
- **Training Time**: 30-60 phÃºt (CPU) 
- **Inference Time**: ~0.1-0.3s/cÃ¢u
- **Accuracy**: ~74% (validation)
