import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
import re
from datetime import datetime
import json
import os
from config import model_config
from reasoning_engine import ReasoningEngine

# Pydantic models
class IntentRequest(BaseModel):
    text: str
    confidence_threshold: Optional[float] = 0.3

class IntentResponse(BaseModel):
    input_text: str
    intent: str
    confidence: float
    command: str
    entities: Dict[str, str]
    value: str
    processing_time: float
    timestamp: str

class SimpleIntentModel(nn.Module):
    """Model t·ªëi ∆∞u cho Intent Recognition v·ªõi Large model v√† GPU"""
    
    def __init__(self, model_name, num_intents, config):
        super().__init__()
        self.config = config
        
        # Load PhoBERT model v·ªõi gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám memory
        self.phobert = AutoModel.from_pretrained(
            model_name,
            gradient_checkpointing=config.gradient_checkpointing
        )
        
        # Multi-layer classifier cho large model
        hidden_size = self.phobert.config.hidden_size
        
        if config.model_size == "large":
            # Large model: s·ª≠ d·ª•ng multi-layer classifier
            self.classifier = nn.Sequential(
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size // 2),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size // 2, hidden_size // 4),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_size // 4),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size // 4, num_intents)
            )
        else:
            # Base model: s·ª≠ d·ª•ng simple classifier
            self.classifier = nn.Sequential(
                nn.Dropout(config.dropout),
                nn.Linear(hidden_size, num_intents)
            )
    
    def forward(self, input_ids, attention_mask):
        # S·ª≠ d·ª•ng gradient checkpointing n·∫øu ƒë∆∞·ª£c b·∫≠t
        if self.config.gradient_checkpointing and self.training:
            outputs = self.phobert(
                input_ids=input_ids, 
                attention_mask=attention_mask,
                use_cache=False  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám memory
            )
        else:
            outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)
        
        # S·ª≠ d·ª•ng mean pooling thay v√¨ pooler_output cho ·ªïn ƒë·ªãnh h∆°n
        sequence_output = outputs.last_hidden_state
        attention_mask_expanded = attention_mask.unsqueeze(-1).float()
        pooled_output = (sequence_output * attention_mask_expanded).sum(dim=1) / attention_mask_expanded.sum(dim=1)
        
        logits = self.classifier(pooled_output)
        return logits

class PhoBERT_SAM_API:
    """API class cho PhoBERT_SAM"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.id_to_intent = None
        self.intent_to_command = None
        
        # Auto-detect device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è Using device: {self.device}")
        
        if self.device.type == "cuda":
            print(f"üéÆ GPU: {torch.cuda.get_device_name()}")
            print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # Intent to Command mapping - Cap nhat theo 28 commands tu dataset
        self.intent_to_command = {
            "adjust-settings": "adjust_settings",
            "app-tutorial": "app_tutorial", 
            "browse-social-media": "browse_social_media",
            "call": "call",
            "check-device-status": "check_device_status",
            "check-health-status": "check_health_status",
            "check-messages": "check_messages",
            "check-weather": "check_weather",
            "control-device": "control_device",
            "general-conversation": "general_conversation",
            "help": "help",
            "make-call": "make_call",
            "make-video-call": "make_video_call",
            "navigation-help": "navigation_help",
            "open-app": "open_app",
            "open-app-action": "open_app_action",
            "play-audio": "play_audio",
            "play-content": "play_content",
            "play-media": "play_media",
            "provide-instructions": "provide_instructions",
            "read-content": "read_content",
            "read-news": "read_news",
            "search-content": "search_content",
            "search-internet": "search_internet",
            "send-message": "send_message",
            "send-mess": "send_mess",
            "set-alarm": "set_alarm",
            "set-reminder": "set_reminder",
            "view-content": "view_content",
            "unknown": "unknown"
        }
        
        # Entity patterns - C·∫£i thi·ªán ƒë·ªÉ extract ch√≠nh x√°c h∆°n
        self.entity_patterns = {
            "RECEIVER": [
                # Improved patterns for receiver extraction
                r"cho\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "cho b√† ngo·∫°i t√¥i ngay b√¢y gi·ªù"
                r"g·ªçi\s+(?:cho|t·ªõi|ƒë·∫øn)?\s*([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "g·ªçi cho b√† ngo·∫°i t√¥i"
                r"nh·∫Øn\s+(?:cho|t·ªõi|ƒë·∫øn)?\s*([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "nh·∫Øn cho b√† ngo·∫°i t√¥i"
                r"g·ª≠i\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "g·ª≠i cho b√°c Lan nh√©"
                r"(?:b√°o|th√¥ng b√°o|n√≥i|n√≥i v·ªõi|th√¥ng tin)\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "th√¥ng b√°o cho ch·ªã H∆∞∆°ng"
                r"(?:s·ªë|s·ªë ƒëi·ªán tho·∫°i|li√™n l·∫°c v·ªõi|li√™n h·ªá v·ªõi)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "li√™n l·∫°c v·ªõi anh Tu·∫•n"
                r"(?:k·∫øt n·ªëi|li√™n l·∫°c|li√™n h·ªá)\s+(?:v·ªõi|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "k·∫øt n·ªëi v·ªõi b√†"
                r"(?:v·ªõi|c√πng)\s+((?:b√°c|ch√∫|c√¥|anh|ch·ªã|em|√¥ng|b√†)\s+[\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "v·ªõi b√°c Ph∆∞∆°ng"
                r"(?:cu·ªôc g·ªçi|g·ªçi ƒëi·ªán|g·ªçi tho·∫°i)\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",  # "cu·ªôc g·ªçi cho b√† ngo·∫°i t√¥i"
                r"(?:th·ª±c hi·ªán|th·ª±c hi·ªán m·ªôt)\s+(?:cu·ªôc g·ªçi|g·ªçi ƒëi·ªán|g·ªçi tho·∫°i)\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])"  # "th·ª±c hi·ªán cu·ªôc g·ªçi cho b√† ngo·∫°i t√¥i"
            ],
            
            "PLATFORM": [
                # Current patterns
                r"tr√™n\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "tr√™n Zalo"
                r"qua\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",
                r"b·∫±ng\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",
                r"(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)\s+app",
                r"app\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",
                r"(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # Tr·ª±c ti·∫øp
                r"v√†o\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "v√†o Facebook"
                
                # Additional patterns
                r"(?:s·ª≠ d·ª•ng|d√πng|th√¥ng qua|qua ƒë∆∞·ªùng|ƒë∆∞·ªùng)\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok|tin nh·∫Øn|ƒëi·ªán tho·∫°i)",  # "s·ª≠ d·ª•ng Zalo"
                r"(?:·ª©ng d·ª•ng|ph·∫ßn m·ªÅm)\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "·ª©ng d·ª•ng Facebook"
                r"(?:m·ªü|v√†o|kh·ªüi ƒë·ªông)\s+(?:·ª©ng d·ª•ng|ph·∫ßn m·ªÅm)?\s*(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "m·ªü ·ª©ng d·ª•ng Zalo"
                r"(?:nh·∫Øn tin|g·ª≠i tin nh·∫Øn|chat)\s+(?:qua|tr√™n|b·∫±ng|d√πng)?\s*(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "nh·∫Øn tin qua Zalo"
                r"(?:g·ªçi|g·ªçi ƒëi·ªán|video call|cu·ªôc g·ªçi|facetime)\s+(?:qua|tr√™n|b·∫±ng|d√πng)?\s*(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "g·ªçi ƒëi·ªán qua Zalo"
                
                # Improved patterns for search and content
                r"(?:t√¨m ki·∫øm|t√¨m|search|tra c·ª©u)\s+(?:tr√™n|qua|b·∫±ng|d√πng)?\s*(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "t√¨m ki·∫øm tr√™n Youtube"
                r"(?:xem|ph√°t|nghe|m·ªü)\s+(?:tr√™n|qua|b·∫±ng|d√πng)?\s*(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)",  # "xem tr√™n Youtube"
                r"(?:v√†o|m·ªü)\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)"  # "v√†o Youtube ƒë·ªÉ t√¨m ki·∫øm"
            ],
            
            "TIME": [
                # Current patterns
                r"(\d{1,2}:\d{2})",
                r"(\d{1,2})\s*gi·ªù",
                r"(\d{1,2})\s*ph√∫t",
                r"(s√°ng|chi·ªÅu|t·ªëi|ƒë√™m)",
                r"(h√¥m\s+nay|ng√†y\s+mai|tu·∫ßn\s+sau)",
                
                # Additional patterns
                r"(\d{1,2})\s*gi·ªù\s*(\d{1,2})?\s*(?:ph√∫t)?",  # "7 gi·ªù 30 ph√∫t", "7 gi·ªù 30", "7 gi·ªù"
                r"(\d{1,2})\s*r∆∞·ª°i",  # "7 r∆∞·ª°i"
                r"(\d{1,2})\s*gi·ªù\s*r∆∞·ª°i",  # "7 gi·ªù r∆∞·ª°i"
                r"(\d{1,2})\s*gi·ªù\s*(?:k√©m|thi·∫øu)\s*(\d{1,2})",  # "7 gi·ªù k√©m 15"
                r"(\d{1,2})\s*gi·ªù\s*(\d{1,2})\s*(?:ph√∫t)?\s*(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m)",  # "7 gi·ªù 30 ph√∫t s√°ng"
                r"(\d{1,2})\s*(?:gi·ªù)?\s*(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m)",  # "7 gi·ªù s√°ng", "7 s√°ng"
                r"(?:l√∫c|v√†o\s+l√∫c|v√†o)\s+(\d{1,2})\s*(?:gi·ªù|h|:)\s*(\d{1,2})?(?:\s*ph√∫t)?",  # "l√∫c 7 gi·ªù", "v√†o l√∫c 7:30"
                r"(?:h√¥m\s+nay|ng√†y\s+mai|ng√†y\s+kia|tu·∫ßn\s+sau|tu·∫ßn\s+t·ªõi)",  # "h√¥m nay", "tu·∫ßn t·ªõi"
                r"(?:th·ª©\s+[Hh]ai|th·ª©\s+[Bb]a|th·ª©\s+[Tt]∆∞|th·ª©\s+[Nn]ƒÉm|th·ª©\s+[Ss]√°u|th·ª©\s+[Bb]·∫£y|ch·ªß\s+nh·∫≠t)",  # "th·ª© hai", "ch·ªß nh·∫≠t"
                r"(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m)\s+(?:nay|mai|kia)",  # "s√°ng nay", "t·ªëi mai"
                r"(?:ng√†y|m√πng|m·ªìng)\s+(\d{1,2})(?:\s+th√°ng\s+(\d{1,2}))?(?:\s+nƒÉm\s+(\d{4}))?",  # "ng√†y 15", "ng√†y 15 th√°ng 8"
                r"(\d{1,2})\/(\d{1,2})(?:\/(\d{4}))?",  # "15/8", "15/8/2023"
                r"(?:v√†i|m·∫•y|m∆∞·ªùi|hai\s+m∆∞∆°i|ba\s+m∆∞∆°i)\s+(?:gi√¢y|ph√∫t|ti·∫øng|ng√†y|tu·∫ßn|th√°ng)\s+(?:t·ªõi|sau|n·ªØa)",  # "v√†i ph√∫t n·ªØa", "m∆∞·ªùi ng√†y t·ªõi"
                r"(?:m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m|ch√≠n|m∆∞·ªùi)\s+(?:gi√¢y|ph√∫t|ti·∫øng|ng√†y|tu·∫ßn|th√°ng)\s+(?:t·ªõi|sau|n·ªØa)"  # "hai ti·∫øng n·ªØa"
            ],
            
            "MESSAGE": [
                # Improved patterns for message extraction
                r"n√≥i\s+r√µ\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "n√≥i r√µ l√† t√¥i mu·ªën tr√≤ chuy·ªán v·ªõi b√†"
                r"r·∫±ng\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "r·∫±ng l√† chi·ªÅu nay ƒë√≥n b√†"
                r"r·∫±ng\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "r·∫±ng chi·ªÅu nay 6 gi·ªù chi·ªÅu ƒë√≥n b√†"
                r"n√≥i\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "n√≥i t√¥i mu·ªën tr√≤ chuy·ªán"
                r"nh·∫Øn\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "nh·∫Øn t√¥i s·∫Ω ƒë·∫øn"
                r"g·ª≠i\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "g·ª≠i t√¥i s·∫Ω ƒë·∫øn"
                
                # Additional patterns
                r"(?:r·∫±ng|l√†)\s+[\"\'](.+?)[\"\']",  # Tr√≠ch d·∫´n n·ªôi dung tin nh·∫Øn b·∫±ng d·∫•u ngo·∫∑c k√©p ho·∫∑c ƒë∆°n
                r"(?:nh·∫Øn|nh·∫Øn tin|g·ª≠i|g·ª≠i tin nh·∫Øn|nh·∫Øn l·∫°i|g·ª≠i l·ªùi nh·∫Øn)\s+(?:r·∫±ng|l√†)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                r"(?:v·ªõi\s+n·ªôi\s+dung|v·ªõi\s+tin\s+nh·∫Øn|tin\s+nh·∫Øn)\s+(?:l√†|r·∫±ng)?\s+[\"\']?(.+?)[\"\']?(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                r"(?:n·ªôi\s+dung|tin\s+nh·∫Øn)\s*[\"\'](.+?)[\"\']",
                r"(?:nh·∫Øn\s+cho\s+\w+\s+)(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # Tin nh·∫Øn sau "nh·∫Øn cho [ng∆∞·ªùi nh·∫≠n]"
                r"(?:g·ª≠i\s+cho\s+\w+\s+)(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"  # Tin nh·∫Øn sau "g·ª≠i cho [ng∆∞·ªùi nh·∫≠n]"
            ],
            
            "LOCATION": [
                # Current patterns
                r"·ªü\s+(\w+(?:\s+\w+)*)",
                r"t·∫°i\s+(\w+(?:\s+\w+)*)",
                r"(\w+(?:\s+\w+)*)\s+(?:th√†nh\s+ph·ªë|t·ªânh|qu·∫≠n|huy·ªán)",
                
                # Additional patterns
                r"(?:·ªü|t·∫°i|trong|ngo√†i|g·∫ßn|xa)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "·ªü H√† N·ªôi", "t·∫°i qu·∫≠n 1"
                r"(?:ƒë·∫øn|t·ªõi|v·ªÅ|qua|sang|ƒëi)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "ƒë·∫øn S√†i G√≤n", "v·ªÅ qu√™"
                r"(?:th√†nh ph·ªë|t·ªânh|qu·∫≠n|huy·ªán|ph∆∞·ªùng|x√£|l√†ng|th√¥n|·∫•p|khu|v√πng)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "th√†nh ph·ªë H·ªì Ch√≠ Minh"
                r"(?:trong|ngo√†i|g·∫ßn|xa)\s+(?:th√†nh ph·ªë|t·ªânh|qu·∫≠n|huy·ªán|ph∆∞·ªùng|x√£|l√†ng|th√¥n|·∫•p|khu|v√πng)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "trong th√†nh ph·ªë ƒê√† N·∫µng"
                r"(?:khu\s+v·ª±c|khu\s+ƒë√¥\s+th·ªã|khu\s+d√¢n\s+c∆∞|l√†ng|x√≥m)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "khu v·ª±c M·ªπ ƒê√¨nh"
                r"(?:ƒë∆∞·ªùng|ph·ªë|ng√µ|ng√°ch|h·∫ªm)\s+([\w\s\d\/]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "ƒë∆∞·ªùng L√™ L·ª£i", "ng√µ 193"
                r"(?:s·ªë\s+nh√†|nh√†\s+s·ªë)\s+([\w\s\d\/]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "s·ªë nh√† 45", "nh√† s·ªë 15"
                r"(?:to√†\s+nh√†|chung\s+c∆∞|khu\s+chung\s+c∆∞|cƒÉn\s+h·ªô)\s+([\w\s\d\/]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "to√† nh√† CT1", "chung c∆∞ Linh ƒê√†m"
                r"(?:b·ªánh\s+vi·ªán|tr∆∞·ªùng\s+h·ªçc|tr∆∞·ªùng|tr∆∞·ªùng\s+ƒë·∫°i\s+h·ªçc|ƒë·∫°i\s+h·ªçc|tr∆∞·ªùng\s+ph·ªï\s+th√¥ng|si√™u\s+th·ªã|ch·ª£|c·ª≠a\s+h√†ng|c√¥ng\s+ty)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"  # "b·ªánh vi·ªán B·∫°ch Mai", "tr∆∞·ªùng ƒë·∫°i h·ªçc Qu·ªëc Gia"
            ],
            
            "APP": [
                # Current patterns
                r"m·ªü\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok|Camera|Gallery|Settings|Clock|Weather|Maps|Calculator)",
                r"v√†o\s+(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok|Camera|Gallery|Settings|Clock|Weather|Maps|Calculator)",
                r"(Facebook|Zalo|Youtube|YouTube|SMS|Messenger|Telegram|Instagram|TikTok|Camera|Gallery|Settings|Clock|Weather|Maps|Calculator)\s+app",
                
                # Additional patterns
                r"(?:m·ªü|kh·ªüi ƒë·ªông|ch·∫°y|v√†o|s·ª≠ d·ª•ng|d√πng)\s+(?:·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm)?\s*(Facebook|Zalo|Youtube|YouTube|TikTok|Instagram|Twitter|Messenger|Viber|Telegram|Google|Gmail|Chrome|Safari|Firefox|Opera|Maps|B·∫£n ƒë·ªì|Tin nh·∫Øn|SMS|ƒêi·ªán tho·∫°i|M√°y t√≠nh|Calculator|Ghi √¢m|Ghi ch√∫|Notes|L·ªãch|Calendar|ƒê·ªìng h·ªì|Clock|B√°o th·ª©c|Alarm|Th·ªùi ti·∫øt|Weather|Camera|M√°y ·∫£nh|Gallery|B·ªô s∆∞u t·∫≠p|H√¨nh ·∫£nh|C√†i ƒë·∫∑t|Settings|Music|Nh·∫°c|Video|Tr√≤ ch∆°i|Game)",
                r"(?:·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm)\s+(Facebook|Zalo|Youtube|YouTube|TikTok|Instagram|Twitter|Messenger|Viber|Telegram|Google|Gmail|Chrome|Safari|Firefox|Opera|Maps|B·∫£n ƒë·ªì|Tin nh·∫Øn|SMS|ƒêi·ªán tho·∫°i|M√°y t√≠nh|Calculator|Ghi √¢m|Ghi ch√∫|Notes|L·ªãch|Calendar|ƒê·ªìng h·ªì|Clock|B√°o th·ª©c|Alarm|Th·ªùi ti·∫øt|Weather|Camera|M√°y ·∫£nh|Gallery|B·ªô s∆∞u t·∫≠p|H√¨nh ·∫£nh|C√†i ƒë·∫∑t|Settings|Music|Nh·∫°c|Video|Tr√≤ ch∆°i|Game)",
                r"(?:v√†o|truy c·∫≠p|s·ª≠ d·ª•ng)\s+(Facebook|Zalo|Youtube|YouTube|TikTok|Instagram|Twitter|Messenger|Viber|Telegram|Google|Gmail|Chrome|Safari|Firefox|Opera|Maps|B·∫£n ƒë·ªì|Tin nh·∫Øn|SMS|ƒêi·ªán tho·∫°i|M√°y t√≠nh|Calculator|Ghi √¢m|Ghi ch√∫|Notes|L·ªãch|Calendar|ƒê·ªìng h·ªì|Clock|B√°o th·ª©c|Alarm|Th·ªùi ti·∫øt|Weather|Camera|M√°y ·∫£nh|Gallery|B·ªô s∆∞u t·∫≠p|H√¨nh ·∫£nh|C√†i ƒë·∫∑t|Settings|Music|Nh·∫°c|Video|Tr√≤ ch∆°i|Game)",
                r"(?:ki·ªÉm tra|xem|theo d√µi)\s+(Facebook|Zalo|Youtube|YouTube|TikTok|Instagram|Twitter|Messenger|Viber|Telegram|Google|Gmail|Chrome|Safari|Firefox|Opera|Maps|B·∫£n ƒë·ªì|Tin nh·∫Øn|SMS|ƒêi·ªán tho·∫°i|M√°y t√≠nh|Calculator|Ghi √¢m|Ghi ch√∫|Notes|L·ªãch|Calendar|ƒê·ªìng h·ªì|Clock|B√°o th·ª©c|Alarm|Th·ªùi ti·∫øt|Weather|Camera|M√°y ·∫£nh|Gallery|B·ªô s∆∞u t·∫≠p|H√¨nh ·∫£nh|C√†i ƒë·∫∑t|Settings|Music|Nh·∫°c|Video|Tr√≤ ch∆°i|Game)",
                r"(?:ch·ª•p ·∫£nh|quay phim|quay video|xem ·∫£nh|xem video)",  # Common app actions that imply app usage
                r"(?:t√≠nh to√°n|t√≠nh|l√†m t√≠nh|t√≠nh nh·∫©m)",  # Calculator
                r"(?:nghe nh·∫°c|ph√°t nh·∫°c|b·∫≠t nh·∫°c)",  # Music app
                r"(?:ghi ch√∫|note|ghi l·∫°i|l∆∞u √Ω)",  # Notes app
                r"(?:ƒë·∫∑t b√°o th·ª©c|h·∫πn gi·ªù|ƒë·∫∑t gi·ªù)",  # Clock/Alarm app
                r"(?:th·ªùi ti·∫øt|d·ª± b√°o|nhi·ªát ƒë·ªô)",  # Weather app
                r"(?:t√¨m ƒë∆∞·ªùng|ch·ªâ ƒë∆∞·ªùng|ƒë·ªãnh v·ªã)"  # Maps app
            ],
            
            "QUERY": [
                # Current patterns
                r"t√¨m\s+(.+)",  # "t√¨m ki·∫øm nh·ªØng th∆∞·ªõc phim h√†i"
                r"t√¨m\s+ki·∫øm\s+(.+)",
                r"search\s+(.+)",
                r"t√¨m\s+video\s+(.+)",
                r"t√¨m\s+nh·∫°c\s+(.+)",
                
                # Additional patterns
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:v·ªÅ|th√¥ng tin v·ªÅ|th√¥ng tin|ki·∫øn th·ª©c v·ªÅ|ki·∫øn th·ª©c)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m ki·∫øm v·ªÅ c√°ch n·∫•u ph·ªü"
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:cho t√¥i|cho m√¨nh|cho t·ªõ|cho b√°c|cho c√¥|cho ch√∫|gi√∫p t√¥i|gi√∫p m√¨nh|gi√∫p b√°c|gi√∫p c√¥|gi√∫p ch√∫)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m cho t√¥i th√¥ng tin v·ªÅ s·ª©c kh·ªèe"
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:video|clip|phim|nh·∫°c|b√†i h√°t|b√†i|album|ca sƒ©|ca kh√∫c|ngh·ªá sƒ©|di·ªÖn vi√™n|t√°c gi·∫£)\s+(?:v·ªÅ|c·ªßa|do|b·ªüi)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m video v·ªÅ c√°ch l√†m b√°nh", "t√¨m nh·∫°c c·ªßa Tr·ªãnh C√¥ng S∆°n"
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:c√¥ng th·ª©c|c√°ch|ph∆∞∆°ng ph√°p|h∆∞·ªõng d·∫´n|ch·ªâ d·∫´n|b√≠ quy·∫øt)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m c√¥ng th·ª©c n·∫•u ƒÉn", "t√¨m c√°ch l√†m b√°nh"
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:tin t·ª©c|th·ªùi s·ª±|b√°o|b·∫£n tin|th√¥ng tin)\s+(?:v·ªÅ|li√™n quan ƒë·∫øn)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m tin t·ª©c v·ªÅ COVID-19"
                r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|tra c·ª©u|ki·∫øm|t√¨m hi·ªÉu)\s+(?:b·ªánh|tri·ªáu ch·ª©ng|thu·ªëc|ƒëi·ªÅu tr·ªã|b√°c sƒ©|y t·∫ø|s·ª©c kh·ªèe)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m tri·ªáu ch·ª©ng b·ªánh ti·ªÉu ƒë∆∞·ªùng"
                r"(?:h·ªèi|tra c·ª©u|tra|h·ªèi v·ªÅ|h·ªèi th√¥ng tin v·ªÅ)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "h·ªèi v·ªÅ c√°ch s·ª≠ d·ª•ng ƒëi·ªán tho·∫°i"
                r"(?:t√¨m hi·ªÉu|nghi√™n c·ª©u|h·ªçc h·ªèi|h·ªçc v·ªÅ)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m hi·ªÉu v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam"
                r"(?:c√°ch|ph∆∞∆°ng ph√°p|l√†m th·∫ø n√†o|l√†m sao|l√†m c√°ch n√†o|l√†m nh∆∞ th·∫ø n√†o ƒë·ªÉ)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "c√°ch n·∫•u ph·ªü", "l√†m th·∫ø n√†o ƒë·ªÉ h·ªçc ti·∫øng Anh"
                
                # Improved patterns for complex search queries
                r"(?:v√†o|m·ªü)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "v√†o youtube ƒë·ªÉ t√¨m ki·∫øm danh s√°ch nh·∫°c"
                r"(?:t√¨m ki·∫øm|t√¨m|search)\s+(?:tr√™n|qua|b·∫±ng|d√πng)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "t√¨m ki·∫øm tr√™n youtube danh s√°ch nh·∫°c"
                r"(?:danh s√°ch|list|playlist)\s+(?:nh·∫°c|music|video|clip|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "danh s√°ch nh·∫°c m·ªõi nh·∫•t"
                r"(?:nh·∫°c|music|video|clip|phim)\s+(?:m·ªõi nh·∫•t|hot|trending|ph·ªï bi·∫øn)\s+(?:c·ªßa|do|b·ªüi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"  # "nh·∫°c m·ªõi nh·∫•t c·ªßa S∆°n T√πng"
            ],
            
            "CONTENT": [
                # Current patterns
                r"phim\s+(.+)",  # "phim h√†i c·ªßa Xu√¢n B·∫Øc"
                r"nh·∫°c\s+(.+)",
                r"video\s+(.+)",
                r"b√†i\s+h√°t\s+(.+)",
                r"tin\s+t·ª©c\s+(.+)",
                
                # Additional patterns
                r"(?:phim|video|clip|nh·∫°c|b√†i h√°t|b√†i|album|ca kh√∫c)\s+(?:v·ªÅ|c·ªßa|do|b·ªüi)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "phim c·ªßa Th√†nh Long", "nh·∫°c c·ªßa Tr·ªãnh C√¥ng S∆°n"
                r"(?:tin t·ª©c|th·ªùi s·ª±|b√°o|b·∫£n tin|th√¥ng tin)\s+(?:v·ªÅ|li√™n quan ƒë·∫øn)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "tin t·ª©c v·ªÅ COVID-19"
                r"(?:ph√°t|b·∫≠t|m·ªü|nghe|xem)\s+(?:phim|video|clip|nh·∫°c|b√†i h√°t|b√†i|album|ca kh√∫c)\s+(?:v·ªÅ|c·ªßa|do|b·ªüi)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "b·∫≠t nh·∫°c c·ªßa S∆°n T√πng", "ph√°t phim h√†i"
                r"(?:ƒë·ªçc|ƒë·ªçc b√°o|ƒë·ªçc tin|ƒë·ªçc tin t·ª©c)\s+(?:v·ªÅ|li√™n quan ƒë·∫øn)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "ƒë·ªçc b√°o v·ªÅ th·ªùi s·ª±"
                r"(?:ca sƒ©|ngh·ªá sƒ©|di·ªÖn vi√™n|nh·∫°c sƒ©|t√°c gi·∫£|ƒë·∫°o di·ªÖn)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "ca sƒ© M·ªπ T√¢m"
                r"(?:th·ªÉ lo·∫°i|lo·∫°i|ki·ªÉu|d·∫°ng)\s+(?:phim|video|clip|nh·∫°c|b√†i h√°t|b√†i|album|ca kh√∫c)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "th·ªÉ lo·∫°i nh·∫°c tr·ªØ t√¨nh"
                r"(?:phim|video|clip|nh·∫°c|b√†i h√°t|b√†i|album|ca kh√∫c)\s+(?:th·ªÉ lo·∫°i|lo·∫°i|ki·ªÉu|d·∫°ng)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",  # "phim th·ªÉ lo·∫°i h√†i"
                r"(?:karaoke|h√°t karaoke)\s+(?:b√†i)?\s*(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"  # "karaoke b√†i ƒê√™m Lao Xao"
            ]
        }
        
        # Kh·ªüi t·∫°o reasoning engine
        self.reasoning_engine = ReasoningEngine()
        print("üß† Reasoning Engine initialized")
    
    def load_model(self):
        """Load trained model v·ªõi h·ªó tr·ª£ large model"""
        try:
            print("üîÑ Loading PhoBERT Large model...")
            
            # Load config
            from config import ModelConfig
            model_config = ModelConfig()
            
            # T√¨m model file - ∆∞u ti√™n best model m·ªõi nh·∫•t
            model_dir = f"models/phobert_{model_config.model_size}_intent_model"
            
            # T√¨m best model m·ªõi nh·∫•t
            best_model_path = None
            if os.path.exists(model_dir):
                best_models = []
                for filename in os.listdir(model_dir):
                    if filename.endswith('.pth') and 'best' in filename:
                        file_path = f"{model_dir}/{filename}"
                        # L·∫•y th√¥ng tin file ƒë·ªÉ t√¨m model m·ªõi nh·∫•t
                        file_time = os.path.getmtime(file_path)
                        best_models.append((file_path, file_time))
                
                if best_models:
                    # S·∫Øp x·∫øp theo th·ªùi gian v√† l·∫•y model m·ªõi nh·∫•t
                    best_models.sort(key=lambda x: x[1], reverse=True)
                    best_model_path = best_models[0][0]
                    print(f"üéØ Found latest best model: {os.path.basename(best_model_path)}")
            
            # N·∫øu kh√¥ng c√≥ best model, t√¨m model th∆∞·ªùng
            if not best_model_path:
                model_path = f"{model_dir}/model.pth"
                if not os.path.exists(model_path):
                    # Fallback to old model
                    model_path = "models/best_simple_intent_model.pth"
                    if not os.path.exists(model_path):
                        print(f"‚ùå No model found, using reasoning engine only")
                        self.model = None
                        self.tokenizer = None
                        self.id_to_intent = None
                        return True
            else:
                model_path = best_model_path
            
            print(f"üìÇ Loading model from: {model_path}")
            
            # Load checkpoint v·ªõi map_location ƒë·ªÉ tr√°nh l·ªói device
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # Ki·ªÉm tra xem c√≥ metadata kh√¥ng
            if 'intent_to_id' not in checkpoint or 'id_to_intent' not in checkpoint:
                print("‚ö†Ô∏è Model kh√¥ng c√≥ metadata, s·ª≠ d·ª•ng reasoning engine only")
                self.model = None
                self.tokenizer = None
                self.id_to_intent = None
                return True
            
            # T·∫°o model v·ªõi config
            self.model = SimpleIntentModel(model_config.model_name, len(checkpoint['intent_to_id']), model_config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            self.model.to(self.device)
            
            # Enable mixed precision n·∫øu c√≥ GPU v√† config cho ph√©p
            if self.device.type == "cuda" and model_config.use_fp16:
                self.model = self.model.half()  # Convert to FP16
                print("üîß Enabled FP16 for GPU inference")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)
            
            # Load mappings
            self.id_to_intent = checkpoint['id_to_intent']
            
            # Load th√¥ng tin model
            model_info = {
                'model_size': checkpoint.get('model_size', 'unknown'),
                'total_parameters': checkpoint.get('total_parameters', 0),
                'trainable_parameters': checkpoint.get('trainable_parameters', 0),
                'epoch': checkpoint.get('epoch', 'unknown'),
                'is_best': checkpoint.get('is_best', False),
                'validation_accuracy': checkpoint.get('validation_accuracy', 'unknown')
            }
            
            print(f"‚úÖ Model loaded successfully from {model_path}")
            print(f"üìä Model file size: {os.path.getsize(model_path) / 1024**2:.2f} MB")
            print(f"üéØ Number of intents: {len(self.id_to_intent)}")
            print(f"üîß Model info: {model_info}")
            print(f"üìã Available intents: {list(self.id_to_intent.values())}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            import traceback
            traceback.print_exc()
            print("üîÑ S·ª≠ d·ª•ng reasoning engine only")
            self.model = None
            self.tokenizer = None
            self.id_to_intent = None
            return True
    
    def extract_entities(self, text: str) -> Dict[str, str]:
        """Extract entities from text with improved logic"""
        entities = {}
        text_lower = text.lower()
        
        # Priority 1: Extract RECEIVER first (most important for call/message)
        receiver_patterns = [
            r"cho\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",
            r"g·ªçi\s+(?:cho|t·ªõi|ƒë·∫øn)?\s*([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",
            r"nh·∫Øn\s+(?:cho|t·ªõi|ƒë·∫øn)?\s*([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",
            r"(?:cu·ªôc g·ªçi|g·ªçi ƒëi·ªán|g·ªçi tho·∫°i)\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])",
            r"(?:th·ª±c hi·ªán|th·ª±c hi·ªán m·ªôt)\s+(?:cu·ªôc g·ªçi|g·ªçi ƒëi·ªán|g·ªçi tho·∫°i)\s+(?:cho|t·ªõi|ƒë·∫øn)\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°|ngay b√¢y gi·ªù|ngay|b√¢y gi·ªù))?(?:$|[\.,])"
        ]
        
        for pattern in receiver_patterns:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match and match.group(1):
                receiver = match.group(1).strip()
                # Validate receiver has relationship terms
                relationship_terms = ["b·ªë", "m·∫π", "√¥ng", "b√†", "anh", "ch·ªã", "em", "con", "ch√°u", "ch√∫", "b√°c", "c√¥", "d√¨", "ngo·∫°i", "n·ªôi"]
                if any(term in receiver for term in relationship_terms):
                    entities["RECEIVER"] = receiver
                    break
        
        # Priority 2: Extract PLATFORM
        platform_patterns = [
            r"b·∫±ng\s+(zalo|facebook|messenger|telegram|instagram|tiktok)",
            r"qua\s+(zalo|facebook|messenger|telegram|instagram|tiktok)",
            r"tr√™n\s+(zalo|facebook|messenger|telegram|instagram|tiktok)",
            r"(zalo|facebook|messenger|telegram|instagram|tiktok)"
        ]
        
        for pattern in platform_patterns:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match and match.group(1):
                entities["PLATFORM"] = match.group(1).lower()
                break
        
        # Priority 3: Extract MESSAGE (only if we have a receiver)
        if "RECEIVER" in entities:
            receiver = entities["RECEIVER"]
            message_patterns = [
                rf"n√≥i\s+r√µ\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                rf"r·∫±ng\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                rf"r·∫±ng\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                rf"n√≥i\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
            ]
            
            for pattern in message_patterns:
                match = re.search(pattern, text_lower, re.IGNORECASE)
                if match and match.group(1):
                    message = match.group(1).strip()
                    # Clean up message - remove redundant words
                    message = re.sub(r'^(?:l√†\s+|r·∫±ng\s+)', '', message)
                    if message and len(message) > 5 and len(message) < 200:  # Reasonable length
                        entities["MESSAGE"] = message
                        break
        
        # Priority 4: Extract TIME (c·∫£i thi·ªán cho nh·∫Øc nh·ªü thu·ªëc)
        time_patterns = [
            # Th·ªùi gian c·ª• th·ªÉ
            r"(\d{1,2})\s*gi·ªù\s*(\d{1,2})?\s*(?:ph√∫t)?",
            r"(\d{1,2})\s*r∆∞·ª°i",
            r"(\d{1,2})\s*(?:gi·ªù|h)\s*(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi)",
            # Th·ªùi gian t∆∞∆°ng ƒë·ªëi
            r"(s√°ng|chi·ªÅu|t·ªëi|ƒë√™m)",
            r"(h√¥m\s+nay|ng√†y\s+mai|tu·∫ßn\s+sau)",
            r"(sau\s+(?:khi\s+)?ƒÉn|sau\s+b·ªØa\s+(?:s√°ng|tr∆∞a|t·ªëi))",
            r"(tr∆∞·ªõc\s+(?:khi\s+)?ƒÉn|tr∆∞·ªõc\s+b·ªØa\s+(?:s√°ng|tr∆∞a|t·ªëi))",
            # Th·ªùi gian ƒë·ªãnh k·ª≥
            r"(h√†ng\s+ng√†y|h√†ng\s+tu·∫ßn|h√†ng\s+th√°ng)",
            r"(th·ª©\s+\d+\s+h√†ng\s+tu·∫ßn)",
            r"(ng√†y\s+\d+\s+h√†ng\s+th√°ng)",
            # Th·ªùi gian ƒëi·ªÅu ki·ªán
            r"(khi\s+c·∫ßn\s+thi·∫øt|khi\s+ƒëau|khi\s+c√≥\s+tri·ªáu\s+ch·ª©ng)",
            # Th·ªùi gian ph·ª©c t·∫°p
            r"(\d{1,2})\s*gi·ªù\s*(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi)\s+v√†\s+(\d{1,2})\s*gi·ªù\s*(?:s√°ng|tr∆∞a|chi·ªÅu|t·ªëi)",
            r"(\d{1,2})\s*l·∫ßn\s+m·ªôt\s+ng√†y:\s*(s√°ng|tr∆∞a|t·ªëi)(?:,\s*(s√°ng|tr∆∞a|t·ªëi))*(?:,\s*(s√°ng|tr∆∞a|t·ªëi))*"
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match:
                if match.groups():
                    time_value = " ".join([g for g in match.groups() if g])
                    if time_value:
                        entities["TIME"] = time_value
                        break
                else:
                    entities["TIME"] = match.group(0)
                    break
        
        # Priority 5: Extract LOCATION (if any)
        location_patterns = [
            r"·ªü\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
            r"t·∫°i\s+([\w\s]+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
        ]
        
        for pattern in location_patterns:
            match = re.search(pattern, text_lower, re.IGNORECASE)
            if match and match.group(1):
                location = match.group(1).strip()
                if location and len(location) > 2:
                    entities["LOCATION"] = location
                    break
        
        # Special case: Check for medicine reminder words (highest priority)
        medicine_words = ["u·ªëng thu·ªëc", "thu·ªëc", "vi√™n thu·ªëc", "kh√°ng sinh", "ti·ªÉu ƒë∆∞·ªùng", "huy·∫øt √°p", "tim", "vitamin", "s·∫Øt", "c·∫£m", "ƒëau ƒë·∫ßu"]
        reminder_words = ["nh·∫Øc", "nh·∫Øc nh·ªü", "ghi nh·ªõ", "reminder", "ƒë·ª´ng qu√™n", "nh·ªõ", "ƒë·∫∑t l·ªùi nh·∫Øc", "ƒë·∫∑t nh·∫Øc nh·ªü"]
        
        if any(word in text_lower for word in medicine_words) and any(word in text_lower for word in reminder_words):
            # This is definitely a medicine reminder
            if "INTENT" not in entities:
                entities["INTENT"] = "set-reminder"
                # Extract medicine action as MESSAGE
                for medicine_word in medicine_words:
                    if medicine_word in text_lower:
                        # Find the full medicine action
                        medicine_patterns = [
                            rf"u·ªëng\s+(\d+\s+)?(?:vi√™n\s+)?{medicine_word}",
                            rf"{medicine_word}\s+(?:l√∫c|v√†o|sau|tr∆∞·ªõc)",
                            rf"u·ªëng\s+{medicine_word}"
                        ]
                        for pattern in medicine_patterns:
                            match = re.search(pattern, text_lower, re.IGNORECASE)
                            if match:
                                if match.groups():
                                    entities["MESSAGE"] = match.group(0)
                                else:
                                    entities["MESSAGE"] = match.group(0)
                                break
                        break
        
        # Special case: Check for message-related words
        elif any(word in text_lower for word in ["nh·∫Øn tin", "g·ª≠i tin", "so·∫°n tin", "text", "sms", "message", "g·ª≠i", "nh·∫Øn"]):
            # This is likely a message sending intent
            if "INTENT" not in entities:
                entities["INTENT"] = "send-mess"
        
        # Special case: If we have a TIME but no specific intent, check for alarm/reminder words
        elif "TIME" in entities:
            if any(word in text_lower for word in ["b√°o th·ª©c", "ƒë√°nh th·ª©c", "alarm", "d·∫≠y", "th·ª©c d·∫≠y"]):
                # This is likely an alarm setting
                if "INTENT" not in entities:
                    entities["INTENT"] = "set-alarm"
            elif any(word in text_lower for word in reminder_words):
                # This is likely a reminder setting
                if "INTENT" not in entities:
                    entities["INTENT"] = "set-reminder"
        
        # Special case: Extract QUERY when search-related words are present
        if "QUERY" not in entities:
            search_words = ["t√¨m", "t√¨m ki·∫øm", "tra c·ª©u", "search", "google"]
            if any(word in text_lower for word in search_words):
                # Try to extract everything after the search word
                for word in search_words:
                    if word in text_lower:
                        start_pos = text_lower.find(word) + len(word)
                        query = text[start_pos:].strip()
                        if query and len(query) > 3:  # Ensure it's not too short
                            entities["QUERY"] = query
                            break
        
        return entities
    
    def generate_value(self, intent: str, entities: Dict[str, str], original_text: str) -> str:
        if intent == "unknown" or intent == "error":
            return "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh"
        
        # T·∫°o value d·ª±a tr√™n intent v√† entities
        if intent in ["call", "make-call", "make-video-call"]:
            receiver = entities.get("RECEIVER", "")
            if not receiver:
                # Try to extract receiver from text if not found by patterns
                potential_receivers = re.findall(r"(?:g·ªçi|g·ªçi cho|g·ªçi ƒëi·ªán cho|nh·∫Øn tin cho|g·ª≠i cho)\s+(\w+(?:\s+\w+){0,2})", original_text, re.IGNORECASE)
                if potential_receivers:
                    receiver = potential_receivers[0]
                else:
                    receiver = "ng∆∞·ªùi nh·∫≠n"
            
            if intent == "make-video-call":
                return f"G·ªçi video cho {receiver}"
            else:
                return f"G·ªçi ƒëi·ªán cho {receiver}"
        
        elif intent in ["send-mess", "send-message"]:
            message = entities.get("MESSAGE", "")
            receiver = entities.get("RECEIVER", "")
            
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p "Ki·ªÉm tra tin nh·∫Øn" b·ªã ph√¢n lo·∫°i sai th√†nh "send-mess"
            if "ki·ªÉm tra" in original_text.lower() and "t·ª´" in original_text.lower():
                match = re.search(r"t·ª´\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if match:
                    from_person = match.group(1).strip()
                    return f"t·ª´ {from_person}"
            
            # ∆Øu ti√™n tr√≠ch xu·∫•t n·ªôi dung ch√≠nh t·ª´ MESSAGE entity
            if message:
                # N·∫øu message ch·ª©a "r·∫±ng l√†" ho·∫∑c t∆∞∆°ng t·ª±, l·∫•y ph·∫ßn sau ƒë√≥
                if "r·∫±ng l√†" in message:
                    content = message.split("r·∫±ng l√†", 1)[-1].strip()
                    # Lo·∫°i b·ªè "l√†" ·ªü ƒë·∫ßu n·∫øu c√≥
                    if content.startswith("l√† "):
                        content = content[3:].strip()
                    return content
                elif message.startswith("l√† "):
                    # N·∫øu message b·∫Øt ƒë·∫ßu b·∫±ng "l√†", lo·∫°i b·ªè n√≥
                    content = message[3:].strip()
                    return content
                elif "r·∫±ng" in message:
                    content = message.split("r·∫±ng", 1)[-1].strip()
                    # Lo·∫°i b·ªè "l√†" ·ªü ƒë·∫ßu n·∫øu c√≥
                    if content.startswith("l√† "):
                        content = content[3:].strip()
                    return content
                elif " l√† " in message:
                    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p "anh Tu·∫•n l√† ƒë√£ nh·∫≠n ƒë∆∞·ª£c ti·ªÅn"
                    content = message.split(" l√† ", 1)[-1].strip()
                    return content
                else:
                    return message
            
            # N·∫øu kh√¥ng c√≥ MESSAGE entity, th·ª≠ tr√≠ch xu·∫•t t·ª´ text g·ªëc
            else:
                # Pattern ƒë·ªÉ t√¨m n·ªôi dung sau "r·∫±ng l√†" ho·∫∑c "r·∫±ng"
                patterns = [
                    r"r·∫±ng\s+l√†\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"r·∫±ng\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫Øn|g·ª≠i|nh·∫Øn tin|g·ª≠i tin nh·∫Øn)(?:\s+cho\s+\w+)?(?:\s+qua\s+\w+)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        content = match.group(1).strip()
                        # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt ·ªü ƒë·∫ßu
                        if content.startswith("l√† "):
                            content = content[3:].strip()
                        return content
                
                # Fallback
                if receiver:
                    return f"Tin nh·∫Øn cho {receiver}"
                else:
                    return "N·ªôi dung tin nh·∫Øn"
        
        elif intent in ["set-alarm", "set-reminder"]:
            time_info = entities.get("TIME", "")
            
            if not time_info:
                # Try to extract time info from text
                time_patterns = [
                    r"(\d{1,2})\s*(?:gi·ªù|h|:)\s*(\d{1,2})?\s*(?:ph√∫t)?",
                    r"(\d{1,2})\s*(?:gi·ªù|h)\s*(?:r∆∞·ª°i|bu·ªïi|s√°ng|tr∆∞a|chi·ªÅu|t·ªëi)",
                    r"(\d{1,2})\s*(?:gi·ªù|h)"
                ]
                
                for pattern in time_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        if match.group(2):  # Hour and minute
                            time_info = f"{match.group(1)}:{match.group(2)}"
                        else:  # Just hour
                            time_info = f"{match.group(1)}:00"
                        break
            
            # Check for period of day if we have time
            if time_info:
                period_match = re.search(r"(s√°ng|tr∆∞a|chi·ªÅu|t·ªëi|ƒë√™m)", original_text, re.IGNORECASE)
                if period_match and period_match.group(1) not in time_info:
                    time_info = f"{time_info} {period_match.group(1)}"
            
            if intent == "set-alarm":
                description = "B√°o th·ª©c"
                description_match = re.search(r"(?:b√°o th·ª©c|alarm)(?:\s+ƒë·ªÉ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if description_match:
                    description = description_match.group(1).strip()
                
                return f"{time_info} - {description}" if time_info else "Th·ªùi gian b√°o th·ª©c"
            else:  # set-reminder
                # C·∫£i thi·ªán cho nh·∫Øc nh·ªü thu·ªëc
                description = "Nh·∫Øc nh·ªü"
                
                # T√¨m ki·∫øm h√†nh ƒë·ªông c·ª• th·ªÉ (ƒë·∫∑c bi·ªát l√† u·ªëng thu·ªëc)
                medicine_patterns = [
                    r"u·ªëng\s+(\d+\s+)?(?:vi√™n\s+)?(?:thu·ªëc\s+)?(?:ti·ªÉu\s+ƒë∆∞·ªùng|huy·∫øt\s+√°p|tim|vitamin|s·∫Øt|c·∫£m|ƒëau\s+ƒë·∫ßu|kh√°ng\s+sinh)",
                    r"(?:thu·ªëc\s+)?(?:ti·ªÉu\s+ƒë∆∞·ªùng|huy·∫øt\s+√°p|tim|vitamin|s·∫Øt|c·∫£m|ƒëau\s+ƒë·∫ßu|kh√°ng\s+sinh)",
                    r"u·ªëng\s+(.+?)(?:\s+(?:l√∫c|v√†o|sau|tr∆∞·ªõc|nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫Øc nh·ªü|nh·∫Øc|reminder)(?:\s+v·ªÅ|v·ªÅ|v·ªÅ vi·ªác|vi·ªác)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in medicine_patterns:
                    description_match = re.search(pattern, original_text, re.IGNORECASE)
                    if description_match:
                        if description_match.groups():
                            description = description_match.group(1).strip()
                        else:
                            description = description_match.group(0).strip()
                        break
                
                return f"{time_info} - {description}" if time_info else description
        
        elif intent == "check-weather":
            location = entities.get("LOCATION", "")
            time = entities.get("TIME", "")
            
            if not location:
                # Try to extract location from text
                location_match = re.search(r"(?:th·ªùi ti·∫øt|nhi·ªát ƒë·ªô|m∆∞a|n·∫Øng)(?:\s+·ªü|t·∫°i|c·ªßa)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if location_match:
                    location = location_match.group(1).strip()
                else:
                    location = "khu v·ª±c hi·ªán t·∫°i"
            
            if time:
                return f"{location} ({time})"
            else:
                return location
        
        elif intent == "check-device-status":
            device = entities.get("DEVICE", "thi·∫øt b·ªã")
            return f"Ki·ªÉm tra tr·∫°ng th√°i {device}"
        
        elif intent == "check-health-status":
            health_aspect = entities.get("HEALTH", "s·ª©c kh·ªèe")
            return f"Ki·ªÉm tra {health_aspect}"
        
        elif intent == "check-messages":
            platform = entities.get("PLATFORM", "")
            receiver = entities.get("RECEIVER", "")
            
            # ∆Øu ti√™n tr√≠ch xu·∫•t "t·ª´ ai" t·ª´ text g·ªëc
            if "t·ª´" in original_text.lower():
                match = re.search(r"t·ª´\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if match:
                    from_person = match.group(1).strip()
                    return f"t·ª´ {from_person}"
            
            # Fallback v·ªõi c√°c entities c√≥ s·∫µn
            if receiver and receiver != "tr√™n":  # "tr√™n" kh√¥ng ph·∫£i ng∆∞·ªùi g·ª≠i
                return f"t·ª´ {receiver}"
            elif platform:
                return f"Ki·ªÉm tra {platform}"
            else:
                return "Ki·ªÉm tra tin nh·∫Øn"
        
        elif intent in ["play-media", "play-audio", "play-content"]:
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            platform = entities.get("PLATFORM", "")
            
            if not content and not query:
                # Try to extract content from text with improved patterns
                content_patterns = [
                    r"(?:ph√°t|m·ªü|b·∫≠t|nghe|xem)(?:\s+b√†i|nh·∫°c|phim|video|clip)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:b√†i h√°t|b√†i|ca kh√∫c|nh·∫°c|phim|video|clip)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m ki·∫øm|t√¨m|search)\s+(?:tr√™n|qua|b·∫±ng|d√πng)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:danh s√°ch|list|playlist)\s+(?:nh·∫°c|music|video|clip|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|music|video|clip|phim)\s+(?:m·ªõi nh·∫•t|hot|trending|ph·ªï bi·∫øn)\s+(?:c·ªßa|do|b·ªüi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in content_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        content = match.group(1).strip()
                        break
            
            # Use query if content is not available
            final_content = content if content else query
            
            if final_content:
                if platform:
                    return f"{final_content} tr√™n {platform}"
                else:
                    return final_content
            else:
                return "N·ªôi dung ph√°t"
        
        elif intent in ["read-news", "read-content"]:
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            
            if not content and not query:
                # Try to extract content/topic from text
                content_match = re.search(r"(?:ƒë·ªçc|ƒë·ªçc tin|ƒë·ªçc b√°o|ƒë·ªçc tin t·ª©c)(?:\s+v·ªÅ|v·ªÅ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if content_match:
                    content = content_match.group(1).strip()
            
            if intent == "read-news":
                if content:
                    return f"Tin t·ª©c v·ªÅ {content}"
                elif query:
                    return f"Tin t·ª©c v·ªÅ {query}"
                else:
                    return "Tin t·ª©c"
            else:  # read-content
                if content:
                    return f"ƒê·ªçc: {content}"
                elif query:
                    return f"ƒê·ªçc v·ªÅ: {query}"
                else:
                    return "N·ªôi dung ƒë·ªçc"
        
        elif intent == "view-content":
            content = entities.get("CONTENT", "")
            query = entities.get("QUERY", "")
            
            if content:
                return f"Xem: {content}"
            elif query:
                return f"Xem v·ªÅ: {query}"
            else:
                return "N·ªôi dung xem"
        
        elif intent == "open-app":
            app = entities.get("APP", "")
            
            if not app:
                # Try to extract app name from text
                app_match = re.search(r"(?:m·ªü|v√†o|kh·ªüi ƒë·ªông|ch·∫°y|s·ª≠ d·ª•ng|d√πng)(?:\s+·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if app_match:
                    app = app_match.group(1).strip()
                else:
                    app = "·ª©ng d·ª•ng"
            
            return app
        
        elif intent == "open-app-action":
            app = entities.get("APP", "·ª©ng d·ª•ng")
            action = entities.get("ACTION", "h√†nh ƒë·ªông")
            return f"{action} trong {app}"
        
        elif intent in ["search-content", "search-internet"]:
            query = entities.get("QUERY", "")
            platform = entities.get("PLATFORM", "")
            
            if not query:
                # Try to extract query from text with improved patterns
                query_patterns = [
                    r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|ki·∫øm|t√¨m hi·ªÉu)(?:\s+v·ªÅ|v·ªÅ|th√¥ng tin v·ªÅ)?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m|t√¨m ki·∫øm|search|tra c·ª©u|tra|ki·∫øm|t√¨m hi·ªÉu)(?:\s+(?:cho t√¥i|cho m√¨nh|cho b√°c|cho c√¥|cho ch√∫|gi√∫p t√¥i|gi√∫p m√¨nh|gi√∫p b√°c|gi√∫p c√¥|gi√∫p ch√∫))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search|ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:t√¨m ki·∫øm|t√¨m|search)\s+(?:tr√™n|qua|b·∫±ng|d√πng)\s+(?:youtube|facebook|zalo|instagram|tiktok)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:danh s√°ch|list|playlist)\s+(?:nh·∫°c|music|video|clip|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|music|video|clip|phim)\s+(?:m·ªõi nh·∫•t|hot|trending|ph·ªï bi·∫øn)\s+(?:c·ªßa|do|b·ªüi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                
                for pattern in query_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        query = match.group(1).strip()
                        break
            
            if query:
                if platform:
                    return f"{query} tr√™n {platform}"
                else:
                    return query
            else:
                return "T·ª´ kh√≥a t√¨m ki·∫øm"
        
        elif intent == "browse-social-media":
            platform = entities.get("PLATFORM", "")
            
            if not platform:
                # Try to extract platform name from text
                platform_match = re.search(r"(?:l∆∞·ªõt|duy·ªát|xem|v√†o|m·ªü)(?:\s+(?:facebook|fb|zalo|instagram|tiktok|youtube|twitter))(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if platform_match:
                    platform = platform_match.group(1).strip()
                else:
                    platform = "m·∫°ng x√£ h·ªôi"
            
            return f"Duy·ªát {platform}"
        
        elif intent == "control-device":
            device = entities.get("DEVICE", "thi·∫øt b·ªã")
            action = entities.get("ACTION", "")
            
            if not action:
                # Try to extract action from text
                action_match = re.search(r"(?:b·∫≠t|t·∫Øt|m·ªü|ƒë√≥ng|kh√≥a|m·ªü kh√≥a|ƒëi·ªÅu ch·ªânh|tƒÉng|gi·∫£m|thay ƒë·ªïi)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if action_match:
                    action = action_match.group(0).strip()  # Use full match as the action
                else:
                    action = "ƒëi·ªÅu khi·ªÉn"
            
            return f"{action} {device}"
        
        elif intent == "adjust-settings":
            setting = entities.get("SETTING", "")
            
            if not setting:
                # Try to extract setting from text
                setting_match = re.search(r"(?:c√†i ƒë·∫∑t|thi·∫øt l·∫≠p|ƒëi·ªÅu ch·ªânh|thay ƒë·ªïi|ch·ªânh|s·ª≠a)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if setting_match:
                    setting = setting_match.group(1).strip()
                else:
                    setting = "c√†i ƒë·∫∑t"
            
            return f"ƒêi·ªÅu ch·ªânh {setting}"
        
        elif intent == "app-tutorial":
            app = entities.get("APP", "")
            
            if not app:
                # Try to extract app name from text
                app_match = re.search(r"(?:h∆∞·ªõng d·∫´n|ch·ªâ d·∫´n|ch·ªâ|d·∫°y|b√†y)(?:\s+(?:s·ª≠ d·ª•ng|d√πng|c√°ch))?(?:\s+(?:·ª©ng d·ª•ng|app|ph·∫ßn m·ªÅm))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if app_match:
                    app = app_match.group(1).strip()
                else:
                    app = "·ª©ng d·ª•ng"
            
            return f"H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng {app}"
        
        elif intent == "navigation-help":
            destination = entities.get("LOCATION", "")
            
            if not destination:
                # Try to extract destination from text
                destination_match = re.search(r"(?:ƒë∆∞·ªùng|ƒë∆∞·ªùng ƒëi|ch·ªâ ƒë∆∞·ªùng|ch·ªâ|ƒëi|t·ªõi|ƒë·∫øn|v·ªÅ)(?:\s+(?:t·ªõi|ƒë·∫øn|v·ªÅ))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if destination_match:
                    destination = destination_match.group(1).strip()
                else:
                    destination = "ƒë√≠ch ƒë·∫øn"
            
            return f"ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn {destination}"
        
        elif intent == "provide-instructions":
            topic = entities.get("TOPIC", "")
            
            if not topic:
                # Try to extract topic from text
                topic_match = re.search(r"(?:h∆∞·ªõng d·∫´n|ch·ªâ d·∫´n|ch·ªâ|d·∫°y|b√†y)(?:\s+(?:v·ªÅ|c√°ch))?\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])", original_text, re.IGNORECASE)
                if topic_match:
                    topic = topic_match.group(1).strip()
                else:
                    topic = "ch·ªß ƒë·ªÅ"
            
            return f"H∆∞·ªõng d·∫´n v·ªÅ {topic}"
        
        elif intent == "general-conversation":
            # Extract general conversational intent
            if "xin ch√†o" in original_text.lower() or "hello" in original_text.lower() or "hi" in original_text.lower():
                return "Ch√†o h·ªèi"
            elif "t·∫°m bi·ªát" in original_text.lower() or "bye" in original_text.lower():
                return "T·∫°m bi·ªát"
            elif "c·∫£m ∆°n" in original_text.lower() or "thanks" in original_text.lower() or "thank" in original_text.lower():
                return "C·∫£m ∆°n"
            elif "xin l·ªói" in original_text.lower() or "sorry" in original_text.lower():
                return "Xin l·ªói"
            elif "kh·ªèe kh√¥ng" in original_text.lower() or "th·∫ø n√†o" in original_text.lower():
                return "H·ªèi thƒÉm"
            else:
                return "Tr√≤ chuy·ªán th√¥ng th∆∞·ªùng"
        
        else:
            # Try to extract meaningful content from text for unknown intents
            if "t√¨m" in original_text.lower() or "t√¨m ki·∫øm" in original_text.lower() or "search" in original_text.lower():
                # Extract search query
                search_patterns = [
                    r"(?:t√¨m|t√¨m ki·∫øm|search)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:v√†o|m·ªü)\s+\w+\s+(?:ƒë·ªÉ|ƒë·ªÉ m√†|m√†)\s+(?:t√¨m|t√¨m ki·∫øm|search)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in search_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            elif "ph√°t" in original_text.lower() or "nghe" in original_text.lower() or "xem" in original_text.lower():
                # Extract media content
                media_patterns = [
                    r"(?:ph√°t|nghe|xem)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:nh·∫°c|video|phim)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in media_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            elif "m·ªü" in original_text.lower() or "v√†o" in original_text.lower():
                # Extract app/platform
                app_patterns = [
                    r"(?:m·ªü|v√†o)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])",
                    r"(?:·ª©ng d·ª•ng|app)\s+(.+?)(?:\s+(?:nh√©|nha|·∫°|nh√°))?(?:$|[\.,])"
                ]
                for pattern in app_patterns:
                    match = re.search(pattern, original_text, re.IGNORECASE)
                    if match:
                        return match.group(1).strip()
            
            # Fallback to generic message
            return f"Th·ª±c hi·ªán h√†nh ƒë·ªông: {intent}"
    
    def predict_intent(self, text: str, confidence_threshold: float = 0.3) -> Dict:
        """Predict intent v√† confidence v·ªõi GPU support"""
        try:
            # Tokenize
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=model_config.max_length,
                return_tensors="pt"
            )
            
            # Move to device
            input_ids = encoding["input_ids"].to(self.device)
            attention_mask = encoding["attention_mask"].to(self.device)
            
            # Convert to FP16 n·∫øu model ƒëang s·ª≠ d·ª•ng FP16
            if self.model.dtype == torch.float16:
                input_ids = input_ids.half()
                attention_mask = attention_mask.half()
            
            # Predict
            with torch.no_grad():
                logits = self.model(input_ids, attention_mask)
                probabilities = torch.softmax(logits, dim=1)
                predicted = torch.argmax(logits, dim=1)
                confidence = probabilities.max().item()
                intent = self.id_to_intent[predicted.item()]
            
            # Check confidence threshold
            if confidence < confidence_threshold:
                intent = "unknown"
                confidence = 0.0
            
            return {
                "intent": intent,
                "confidence": confidence,
                "probabilities": probabilities.cpu().numpy().tolist()[0]
            }
            
        except Exception as e:
            print(f"‚ùå Error predicting intent: {e}")
            return {
                "intent": "error",
                "confidence": 0.0,
                "probabilities": []
            }
    
    async def predict_with_reasoning(self, text: str) -> Dict[str, Any]:
        """Predict intent v·ªõi reasoning engine cho c√°c t·ª´ ng·ªØ kh√¥ng c√≥ trong dataset - C·∫£i ti·∫øn"""
        try:
            print(f"üß† REASONING PREDICTION: '{text}'")
            start_time = datetime.now()
            
            # 1. Th·ª≠ predict v·ªõi model ƒë√£ train tr∆∞·ªõc (n·∫øu c√≥)
            try:
                model_result = None
                model_confidence = 0.0
                model_intent = "unknown"
                
                if self.model and self.tokenizer:
                    model_result = self.predict_intent(text)
                    model_confidence = model_result.get("confidence", 0.0)
                    model_intent = model_result.get("intent", "unknown")
                    print(f"ü§ñ MODEL PREDICTION: {model_intent} (confidence: {model_confidence:.3f})")
            except Exception as e:
                print(f"‚ö†Ô∏è Model prediction error: {str(e)}")
                model_result = {"intent": "unknown", "confidence": 0.0}
                model_confidence = 0.0
                model_intent = "unknown"
            
            # 2. N·∫øu confidence th·∫•p ho·∫∑c intent l√† unknown, s·ª≠ d·ª•ng reasoning
            if model_confidence < 0.6 or model_intent == "unknown":
                print("üîç Confidence th·∫•p ho·∫∑c intent unknown, s·ª≠ d·ª•ng Reasoning Engine")
                
                # Ki·ªÉm tra t·ª´ kh√≥a nh·∫Øn tin tr∆∞·ªõc khi d√πng reasoning
                text_lower = text.lower()
                message_keywords = ["nh·∫Øn tin", "g·ª≠i tin", "so·∫°n tin", "text", "sms", "message", "g·ª≠i", "nh·∫Øn"]
                has_message_keyword = any(keyword in text_lower for keyword in message_keywords)
                
                reasoning_result = self.reasoning_engine.reasoning_predict(text)
                reasoning_intent = reasoning_result.get("intent", "unknown")
                reasoning_confidence = reasoning_result.get("confidence", 0.0)
                print(f"üß† REASONING PREDICTION: {reasoning_intent} (confidence: {reasoning_confidence:.3f})")
                
                # ∆Øu ti√™n call n·∫øu c√≥ t·ª´ kh√≥a g·ªçi ƒëi·ªán
                call_keywords = ["cu·ªôc g·ªçi", "g·ªçi tho·∫°i", "g·ªçi ƒëi·ªán", "th·ª±c hi·ªán g·ªçi", "th·ª±c hi·ªán cu·ªôc g·ªçi"]
                has_call_keyword = any(keyword in text_lower for keyword in call_keywords)
                
                if has_call_keyword and reasoning_intent != "call":
                    print("üîß Override intent to call due to call keywords")
                    reasoning_intent = "call"
                    reasoning_confidence = max(reasoning_confidence, 0.8)  # Boost confidence
                
                # ∆Øu ti√™n send-mess n·∫øu c√≥ t·ª´ kh√≥a nh·∫Øn tin
                elif has_message_keyword and reasoning_intent != "send-mess":
                    print("üîß Override intent to send-mess due to message keywords")
                    reasoning_intent = "send-mess"
                    reasoning_confidence = max(reasoning_confidence, 0.7)  # Boost confidence
                
                # Decide which intent to use (model or reasoning)
                final_intent = reasoning_intent
                final_confidence = reasoning_confidence
                method = "reasoning_engine"
                
                # If model had a non-unknown prediction with reasonable confidence, compare
                if model_intent != "unknown" and model_confidence >= 0.4:
                    # Check if model and reasoning agree
                    if model_intent == reasoning_intent:
                        # Both agree, boost confidence
                        final_confidence = max(model_confidence, reasoning_confidence) + 0.1
                        final_confidence = min(final_confidence, 0.99)  # Cap at 0.99
                        method = "model_reasoning_agreement"
                    else:
                        # They disagree, use the one with higher confidence
                        if model_confidence > reasoning_confidence + 0.2:  # Model significantly more confident
                            final_intent = model_intent
                            final_confidence = model_confidence
                            method = "model_override"
                        # Otherwise, stick with reasoning (default above)
            else:
                # Model has high confidence, use its prediction
                final_intent = model_intent
                final_confidence = model_confidence
                method = "trained_model"
                reasoning_result = None
            
            # Extract entities using our improved method
            try:
                entities = self.extract_entities(text)
            except Exception as e:
                print(f"‚ö†Ô∏è Error extracting entities: {e}")
                entities = {}
            
            # Get command from intent
            command = self.intent_to_command.get(final_intent, "unknown")
            
            # Generate value from intent and entities
            try:
                value = self.generate_value(final_intent, entities, text)
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating value: {e}")
                value = f"Th·ª±c hi·ªán h√†nh ƒë·ªông: {final_intent}"
            
            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Construct and return the final result
            result = {
                "text": text,
                "intent": final_intent,
                "confidence": final_confidence,
                "command": command,
                "entities": entities,
                "value": value,
                "method": method,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat()
            }
            
            # Add reasoning details if available (simplified)
            if reasoning_result:
                result["reasoning_details"] = {
                    "semantic_similarity": reasoning_result.get("semantic_similarity", {})
                }
            
            return result
                    
        except Exception as e:
            print(f"‚ùå Error in reasoning prediction: {str(e)}")
            import traceback
            print(f"üîç Full traceback: {traceback.format_exc()}")
            return {
                "text": text,
                "intent": "unknown",
                "confidence": 0.0,
                "command": "unknown",
                "entities": {},
                "value": "",
                "method": "error",
                "error": str(e),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
    
    async def batch_predict_with_reasoning(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Batch predict v·ªõi reasoning engine"""
        results = []
        for text in texts:
            result = await self.predict_with_reasoning(text)
            results.append(result)
        return results
    
    async def analyze_text_semantics(self, text: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch semantic c·ªßa text"""
        try:
            # L·∫•y embedding
            embedding = self.reasoning_engine.get_text_embedding(text)
            
            # T√¨m similar intents
            similar_intents = self.reasoning_engine.find_similar_intents(text)
            
            # Extract context features
            context_features = self.reasoning_engine.extract_context_features(text)
            
            # Pattern matching
            pattern_results = self.reasoning_engine.pattern_matching(text)
            
            # Keyword matching
            keyword_results = self.reasoning_engine.keyword_matching(text)
            
            return {
                "text": text,
                "embedding_shape": embedding.shape,
                "similar_intents": similar_intents,
                "context_features": context_features,
                "pattern_matching": pattern_results,
                "keyword_matching": keyword_results,
                "semantic_analysis": {
                    "has_time_context": context_features["has_time"],
                    "has_person_context": context_features["has_person"],
                    "has_action_context": context_features["has_action"],
                    "has_object_context": context_features["has_object"]
                }
            }
            
        except Exception as e:
            return {
                "text": text,
                "error": str(e)
            }
    
    async def update_knowledge_base(self, new_patterns: Dict[str, List[str]]) -> Dict[str, Any]:
        """C·∫≠p nh·∫≠t knowledge base c·ªßa reasoning engine"""
        try:
            self.reasoning_engine.update_knowledge_base(new_patterns)
            return {
                "status": "success",
                "message": "Knowledge base updated successfully",
                "updated_patterns": new_patterns
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def save_knowledge_base(self, file_path: str = "knowledge_base.json") -> Dict[str, Any]:
        """L∆∞u knowledge base"""
        try:
            self.reasoning_engine.save_knowledge_base(file_path)
            return {
                "status": "success",
                "message": f"Knowledge base saved to {file_path}",
                "file_path": file_path
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def load_knowledge_base(self, file_path: str = "knowledge_base.json") -> Dict[str, Any]:
        """Load knowledge base"""
        try:
            self.reasoning_engine.load_knowledge_base(file_path)
            return {
                "status": "success",
                "message": f"Knowledge base loaded from {file_path}",
                "file_path": file_path
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

    async def process_text(self, text: str, confidence_threshold: float = 0.3) -> IntentResponse:
        """X·ª≠ l√Ω text v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß"""
        start_time = datetime.now()
        
        # Predict intent - kh√¥ng c·∫ßn await v√¨ predict_intent kh√¥ng ph·∫£i async
        intent_result = self.predict_intent(text, confidence_threshold)
        
        # Extract entities
        entities = self.extract_entities(text)
        
        # Get command
        command = self.intent_to_command.get(intent_result["intent"], "unknown")
        
        # Generate value based on intent and entities
        value = self.generate_value(intent_result["intent"], entities, text)
        
        # Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return IntentResponse(
            input_text=text,
            intent=intent_result["intent"],
            confidence=intent_result["confidence"],
            command=command,
            entities=entities,
            value=value,
            processing_time=processing_time,
            timestamp=datetime.now().isoformat()
        )

# Initialize API
api = PhoBERT_SAM_API()

# FastAPI app
app = FastAPI(
    title="PhoBERT_SAM API",
    description="API cho Intent Recognition v√† Entity Extraction cho ng∆∞·ªùi cao tu·ªïi",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins (c√≥ th·ªÉ thay ƒë·ªïi th√†nh specific domains)
    allow_credentials=True,
    allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ HTTP methods
    allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers
)

@app.on_event("startup")
async def startup_event():
    """Kh·ªüi t·∫°o model khi server start"""
    if not api.load_model():
        raise Exception("Kh√¥ng th·ªÉ load model!")

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "PhoBERT_SAM API",
        "version": "1.0.0",
        "status": "running",
        "available_intents": list(api.intent_to_command.keys()) if api.id_to_intent else []
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": api.model is not None,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict")
async def predict_intent(request: IntentRequest):
    """Predict intent v√† extract entities - Simplified response"""
    try:
        # N·∫øu model kh√¥ng c√≥, s·ª≠ d·ª•ng reasoning engine
        if not api.model:
            reasoning_result = await api.predict_with_reasoning(request.text)
            
            # Return simplified response
            return {
                "text": request.text,
                "intent": reasoning_result["intent"],
                "confidence": reasoning_result["confidence"],
                "command": reasoning_result["command"],
                "entities": reasoning_result["entities"],
                "value": reasoning_result["value"],
                "method": reasoning_result.get("method", "reasoning_engine"),
                "processing_time": reasoning_result["processing_time"],
                "timestamp": reasoning_result["timestamp"],
                "reasoning_details": reasoning_result.get("reasoning_details", {})
            }
        
        # N·∫øu c√≥ model, s·ª≠ d·ª•ng process_text
        result = await api.process_text(request.text, request.confidence_threshold)
        
        # Convert to simplified format
        return {
            "text": result.input_text,
            "intent": result.intent,
            "confidence": result.confidence,
            "command": result.command,
            "entities": result.entities,
            "value": result.value,
            "method": "trained_model",
            "processing_time": result.processing_time,
            "timestamp": result.timestamp
        }
        
    except Exception as e:
        print(f"‚ùå Error in predict endpoint: {str(e)}")
        
        # Return simplified error response
        return {
            "text": request.text,
            "intent": "unknown",
            "confidence": 0.0,
            "command": "unknown",
            "entities": {},
            "value": "",
            "method": "error",
            "processing_time": 0.0,
            "timestamp": datetime.now().isoformat()
        }

@app.get("/intents")
async def get_intents():
    """L·∫•y danh s√°ch intents c√≥ s·∫µn"""
    if not api.id_to_intent:
        raise HTTPException(status_code=500, detail="Model ch∆∞a ƒë∆∞·ª£c load")
    
    return {
        "intents": list(api.id_to_intent.values()),
        "intent_to_command": api.intent_to_command
    }

@app.get("/entities")
async def get_entity_patterns():
    """L·∫•y patterns cho entity extraction"""
    return {
        "entity_patterns": api.entity_patterns
    }

@app.post("/batch_predict")
async def batch_predict(texts: List[str], confidence_threshold: float = 0.3):
    """Predict nhi·ªÅu texts c√πng l√∫c"""
    try:
        if not api.model:
            raise HTTPException(status_code=500, detail="Model ch∆∞a ƒë∆∞·ª£c load")
        
        results = []
        for text in texts:
            result = await api.process_text(text, confidence_threshold)
            results.append(result.dict())
        
        return {
            "results": results,
            "total_processed": len(texts)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω batch: {str(e)}")

# Th√™m c√°c endpoints m·ªõi cho reasoning
@app.post("/predict-with-reasoning")
async def predict_with_reasoning(request: Dict[str, Any]):
    """Predict intent v·ªõi reasoning engine"""
    text = request.get("text", "")
    if not text:
        return {"error": "Text is required"}
    
    result = await api.predict_with_reasoning(text)
    return result

@app.post("/batch-predict-with-reasoning")
async def batch_predict_with_reasoning(request: Dict[str, Any]):
    """Batch predict v·ªõi reasoning engine"""
    texts = request.get("texts", [])
    if not texts:
        return {"error": "Texts list is required"}
    
    results = await api.batch_predict_with_reasoning(texts)
    return {"results": results}

@app.post("/analyze-semantics")
async def analyze_semantics(request: Dict[str, Any]):
    """Ph√¢n t√≠ch semantic c·ªßa text"""
    text = request.get("text", "")
    if not text:
        return {"error": "Text is required"}
    
    result = await api.analyze_text_semantics(text)
    return result

@app.post("/update-knowledge-base")
async def update_knowledge_base(request: Dict[str, Any]):
    """C·∫≠p nh·∫≠t knowledge base"""
    new_patterns = request.get("patterns", {})
    if not new_patterns:
        return {"error": "Patterns are required"}
    
    result = await api.update_knowledge_base(new_patterns)
    return result

@app.post("/save-knowledge-base")
async def save_knowledge_base(request: Dict[str, Any]):
    """L∆∞u knowledge base"""
    file_path = request.get("file_path", "knowledge_base.json")
    result = await api.save_knowledge_base(file_path)
    return result

@app.post("/load-knowledge-base")
async def load_knowledge_base(request: Dict[str, Any]):
    """Load knowledge base"""
    file_path = request.get("file_path", "knowledge_base.json")
    result = await api.load_knowledge_base(file_path)
    return result

if __name__ == "__main__":
    print("üöÄ Starting PhoBERT_SAM API Server...")
    print("=" * 50)
    print("üìñ API Documentation: http://localhost:8000/docs")
    print("üîç Health Check: http://localhost:8000/health")
    print("üéØ Predict Endpoint: POST http://localhost:8000/predict")
    print("=" * 50)
    
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
