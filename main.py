#!/usr/bin/env python3
"""
Script ch√≠nh cho d·ª± √°n PhoBERT_SAM
Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh Intent Recognition, Entity Extraction v√† Command Processing
"""

import argparse
import torch
import numpy as np
from data import DataProcessor
from training import Trainer
from models import create_model
from config import training_config, model_config, command_config, intent_config
from utils import extract_entities_from_predictions, format_prediction_output
from reasoning_engine import ReasoningEngine
import json

def set_seed(seed: int):
    """Set random seed ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

def setup_device():
    """Setup device v√† GPU settings"""
    # Auto-detect device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è Using device: {device}")
    
    if device.type == "cuda":
        print(f"üéÆ GPU: {torch.cuda.get_device_name()}")
        print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # Enable cuDNN benchmark cho t·ªëi ∆∞u performance
        torch.backends.cudnn.benchmark = True
        print("üöÄ Enabled cuDNN benchmark for optimal performance")
        
        # Set memory fraction n·∫øu c·∫ßn
        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
            torch.cuda.set_per_process_memory_fraction(0.9)  # S·ª≠ d·ª•ng 90% GPU memory
            print("üíæ Set GPU memory fraction to 90%")
    
    return device

def train_models():
    """Hu·∫•n luy·ªán c√°c m√¥ h√¨nh Intent Recognition, Entity Extraction v√† Command Processing cho ng∆∞·ªùi cao tu·ªïi"""
    print("=== B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH PHOBERT_SAM CHO NG∆Ø·ªúI CAO TU·ªîI ===")
    
    # Setup device v√† GPU
    device = setup_device()
    
    # Set seed
    set_seed(training_config.seed)
    
    # Kh·ªüi t·∫°o data processor
    data_processor = DataProcessor()
    
    # T·∫£i dataset cho ng∆∞·ªùi cao tu·ªïi
    print("ƒêang t·∫£i dataset cho ng∆∞·ªùi cao tu·ªïi...")
    dataset = data_processor.load_dataset("elderly_command_dataset_reduced.json")
    print(f"ƒê√£ t·∫£i {len(dataset)} m·∫´u d·ªØ li·ªáu cho ng∆∞·ªùi cao tu·ªïi")
    
    # Data augmentation cho Intent Recognition
    print("Th·ª±c hi·ªán data augmentation cho Intent Recognition...")
    augmented_dataset = data_processor.augment_intent_data(dataset, augmentation_factor=0.3)
    print(f"Sau augmentation: {len(augmented_dataset)} m·∫´u d·ªØ li·ªáu")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Intent Recognition v·ªõi confidence
    print("Chu·∫©n b·ªã d·ªØ li·ªáu cho Intent Recognition n√¢ng cao...")
    intent_data = data_processor.prepare_intent_data_with_confidence(augmented_dataset)
    train_intent, val_intent = data_processor.split_dataset(intent_data)
    print(f"Intent data - Train: {len(train_intent)}, Val: {len(val_intent)}")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Entity Extraction
    print("Chu·∫©n b·ªã d·ªØ li·ªáu cho Entity Extraction...")
    entity_data = data_processor.prepare_entity_data(dataset)
    train_entity, val_entity = data_processor.split_dataset(entity_data)
    print(f"Entity data - Train: {len(train_entity)}, Val: {len(val_entity)}")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho Command Processing
    print("Chu·∫©n b·ªã d·ªØ li·ªáu cho Command Processing...")
    command_data = data_processor.prepare_command_data(dataset)
    train_command, val_command = data_processor.split_dataset(command_data)
    print(f"Command data - Train: {len(train_command)}, Val: {len(val_command)}")
    
    # Hu·∫•n luy·ªán Intent Recognition Model n√¢ng cao
    print("\n=== HU·∫§N LUY·ªÜN INTENT RECOGNITION MODEL N√ÇNG CAO ===")
    print(f"S·ª≠ d·ª•ng confidence threshold: {intent_config.confidence_threshold}")
    print(f"S·ªë intent classes: {intent_config.num_intents}")
    print(f"Intent labels: {intent_config.intent_labels}")
    
    trainer = Trainer()
    intent_model = trainer.train_intent_model_with_confidence(
        train_intent, val_intent
    )
    
    # Hu·∫•n luy·ªán Entity Extraction Model
    print("\n=== HU·∫§N LUY·ªÜN ENTITY EXTRACTION MODEL ===")
    print(f"S·ªë entity classes: {command_config.num_commands}")
    print(f"Entity labels: {command_config.command_labels}")
    
    entity_model = trainer.train_entity_model(
        train_entity, val_entity
    )
    
    # Hu·∫•n luy·ªán Command Processing Model
    print("\n=== HU·∫§N LUY·ªÜN COMMAND PROCESSING MODEL ===")
    print(f"S·ªë command classes: {command_config.num_commands}")
    print(f"Command labels: {command_config.command_labels}")
    
    command_model = trainer.train_command_model(
        train_command, val_command
    )
    
    # Hu·∫•n luy·ªán Unified Model
    print("\n=== HU·∫§N LUY·ªÜN UNIFIED MODEL ===")
    print("K·∫øt h·ª£p c·∫£ 3 t√°c v·ª•: Intent + Entity + Command")
    
    unified_model = trainer.train_unified_model(
        train_intent, val_intent,
        train_entity, val_entity,
        train_command, val_command
    )
    
    print("\n=== HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN ===")
    print("T·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng!")
    print("C√°c m√¥ h√¨nh ƒë∆∞·ª£c t·ªëi ∆∞u cho ng∆∞·ªùi cao tu·ªïi v·ªõi 9 t√°c v·ª• c∆° b·∫£n:")
    print("1. G·ªçi ƒëi·ªán tho·∫°i (call)")
    print("2. G·ª≠i tin nh·∫Øn (send-mess)")
    print("3. Xem video/nghe nh·∫°c (play-media)")
    print("4. Ki·ªÉm tra th·ªùi ti·∫øt (check-weather)")
    print("5. ƒê·∫∑t b√°o th·ª©c (set-alarm)")
    print("6. Nh·∫Øc nh·ªü thu·ªëc (set-reminder)")
    print("7. ƒê·ªçc tin t·ª©c (read-news)")
    print("8. Ki·ªÉm tra s·ª©c kh·ªèe (check-health-status)")
    print("9. Tr√≤ chuy·ªán chung (general-conversation)")

def test_models():
    """Test c√°c m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v·ªõi confidence validation"""
    print("=== TESTING M√î H√åNH PHOBERT_SAM N√ÇNG CAO ===")
    
    # Kh·ªüi t·∫°o data processor
    data_processor = DataProcessor()
    
    # T·∫£i dataset
    dataset = data_processor.load_dataset("nlp_command_dataset.json")
    
    # Test Intent Recognition n√¢ng cao
    print("\n--- Test Intent Recognition n√¢ng cao ---")
    intent_model = create_model("intent")
    intent_model.load_state_dict(torch.load(f"{training_config.output_dir}/best_intent_model.pth"))
    intent_model.eval()
    
    # Test m·ªôt s·ªë m·∫´u v·ªõi confidence validation
    test_samples = [
        "nh·∫Øc t√¥i l√∫c 5 gi·ªù chi·ªÅu",
        "alo cho b·ªë",
        "g·ª≠i tin nh·∫Øn cho m·∫π: y√™u m·∫π nhi·ªÅu",
        "ƒë·∫∑t b√°o th·ª©c l√∫c 7 gi·ªù s√°ng",
        "g·ªçi Minh ngay",
        "th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o",
        "t√¥i mu·ªën nghe nh·∫°c",
        "ƒë·ªçc tin t·ª©c cho t√¥i",
        "t√¥i c·∫£m th·∫•y m·ªát m·ªèi",
        "h√¥m nay tr·ªùi ƒë·∫πp qu√°"
    ]
    
    for text in test_samples:
        # Tokenize
        encoding = data_processor.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=model_config.max_length,
            return_tensors="pt"
        )
        
        # Predict v·ªõi confidence
        predicted_intents, confidences = intent_model.predict_with_confidence(
            encoding["input_ids"], 
            encoding["attention_mask"], 
            intent_config.confidence_threshold
        )
        
        # Validate prediction
        if predicted_intents[0] != -1:
            predicted_intent = intent_config.intent_labels[predicted_intents[0]]
            confidence = confidences[0].item()
            
            # Validation v·ªõi rule-based checks
            validation = data_processor.validate_intent_prediction(
                predicted_intent, confidence, text
            )
            
            print(f"\nText: '{text}'")
            print(f"Predicted Intent: {predicted_intent}")
            print(f"Confidence: {confidence:.3f}")
            print(f"Validation: {'‚úì' if validation['is_valid'] else '‚úó'}")
            
            if validation['warnings']:
                print(f"Warnings: {', '.join(validation['warnings'])}")
            if validation['suggestions']:
                print(f"Suggestions: {', '.join(validation['suggestions'])}")
        else:
            print(f"\nText: '{text}'")
            print("Predicted Intent: UNKNOWN (confidence too low)")
            print(f"Confidence: {confidences[0].item():.3f}")
    
    # Test Entity Extraction
    print("\n--- Test Entity Extraction ---")
    entity_model = create_model("entity")
    entity_model.load_state_dict(torch.load(f"{training_config.output_dir}/best_entity_model.pth"))
    entity_model.eval()
    
    test_sample = "g·ª≠i tin nh·∫Øn cho Minh r·∫±ng y√™u m·∫π nhi·ªÅu"
    encoding = data_processor.tokenizer(
        test_sample,
        truncation=True,
        padding="max_length",
        max_length=model_config.max_length,
        return_tensors="pt"
    )
    
    with torch.no_grad():
        logits = entity_model(encoding["input_ids"], encoding["attention_mask"])
        predictions = torch.argmax(logits, dim=2)
    
    # Decode predictions
    tokens = data_processor.tokenizer.tokenize(test_sample)
    predicted_labels = predictions[0][:len(tokens)].cpu().numpy()
    
    print(f"Input: {test_sample}")
    print("Token-level predictions:")
    for token, label_id in zip(tokens, predicted_labels):
        label = data_processor.entity_id2label[label_id]
        print(f"  {token}: {label}")
    print("-" * 50)
    
    # Test Command Processing
    print("\n--- Test Command Processing ---")
    command_model = create_model("command")
    command_model.load_state_dict(torch.load(f"{training_config.output_dir}/best_command_model.pth"))
    command_model.eval()
    
    for sample in test_samples:
        encoding = data_processor.tokenizer(
            sample,
            truncation=True,
            padding="max_length",
            max_length=model_config.max_length,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            logits = command_model(encoding["input_ids"], encoding["attention_mask"])
            predicted = torch.argmax(logits, dim=1)
            command = data_processor.command_id2label[predicted.item()]
        
        print(f"Input: {sample}")
        print(f"Predicted Command: {command}")
        print("-" * 50)
    
    # Test Unified Model
    print("\n--- Test Unified Model ---")
    unified_model = create_model("unified")
    unified_model.load_state_dict(torch.load(f"{training_config.output_dir}/best_unified_model.pth"))
    unified_model.eval()
    
    test_sample = "g·ª≠i tin nh·∫Øn cho Minh r·∫±ng y√™u m·∫π nhi·ªÅu"
    encoding = data_processor.tokenizer(
        test_sample,
        truncation=True,
        padding="max_length",
        max_length=model_config.max_length,
        return_tensors="pt"
    )
    
    with torch.no_grad():
        intent_logits, entity_logits, command_logits = unified_model(encoding["input_ids"], encoding["attention_mask"])
        
        # Intent prediction
        intent_pred = torch.argmax(intent_logits, dim=1)
        intent = data_processor.intent_id2label[intent_pred.item()]
        
        # Command prediction
        command_pred = torch.argmax(command_logits, dim=1)
        command = data_processor.command_id2label[command_pred.item()]
        
        # Entity prediction
        entity_preds = torch.argmax(entity_logits, dim=2)
        tokens = data_processor.tokenizer.tokenize(test_sample)
        predicted_labels = entity_preds[0][:len(tokens)].cpu().numpy()
        
        # Extract entities
        entities = extract_entities_from_predictions(tokens, predicted_labels, data_processor.entity_id2label)
    
    print(f"Input: {test_sample}")
    print(f"Intent: {intent}")
    print(f"Command: {command}")
    print("Entities:")
    for entity in entities:
        print(f"  - {entity['type']}: '{entity['text']}'")
    print("-" * 50)

def predict_command(text: str):
    """D·ª± ƒëo√°n command cho m·ªôt c√¢u input v·ªõi reasoning engine fallback"""
    print(f"=== D·ª∞ ƒêO√ÅN COMMAND CHO: '{text}' ===")
    
    # Kh·ªüi t·∫°o data processor v√† reasoning engine
    data_processor = DataProcessor()
    reasoning_engine = ReasoningEngine()
    
    try:
        # Load Unified Model
        unified_model = create_model("unified")
        unified_model.load_state_dict(torch.load(f"{training_config.output_dir}/best_unified_model.pth"))
        unified_model.eval()
        
        # Encode text
        encoding = data_processor.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=model_config.max_length,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            intent_logits, entity_logits, command_logits = unified_model(encoding["input_ids"], encoding["attention_mask"])
            
            # Intent prediction
            intent_pred = torch.argmax(intent_logits, dim=1)
            intent = data_processor.intent_id2label[intent_pred.item()]
            intent_confidence = torch.softmax(intent_logits, dim=1).max().item()
            
            # Command prediction
            command_pred = torch.argmax(command_logits, dim=1)
            command = data_processor.command_id2label[command_pred.item()]
            
            # Entity prediction
            entity_preds = torch.argmax(entity_logits, dim=2)
            tokens = data_processor.tokenizer.tokenize(text)
            predicted_labels = entity_preds[0][:len(tokens)].cpu().numpy()
            
            # Extract entities
            entities = extract_entities_from_predictions(tokens, predicted_labels, data_processor.entity_id2label)
        
        # Ki·ªÉm tra confidence v√† s·ª≠ d·ª•ng reasoning engine n·∫øu c·∫ßn
        if intent_confidence < intent_config.confidence_threshold or intent == "unknown":
            print(f"‚ö†Ô∏è Model confidence th·∫•p ({intent_confidence:.3f}), s·ª≠ d·ª•ng reasoning engine...")
            reasoning_result = reasoning_engine.reasoning_predict(text)
            
            result = {
                "input": text,
                "intent": reasoning_result["intent"],
                "command": reasoning_result.get("command", "unknown"),
                "entities": entities,
                "confidence": reasoning_result["confidence"],
                "method": "reasoning_engine",
                "explanation": reasoning_result.get("explanation", "")
            }
        else:
            result = {
                "input": text,
                "intent": intent,
                "command": command,
                "entities": entities,
                "confidence": intent_confidence,
                "method": "trained_model"
            }
    
    except Exception as e:
        print(f"‚ùå L·ªói khi s·ª≠ d·ª•ng trained model: {str(e)}")
        print("üîÑ Chuy·ªÉn sang reasoning engine...")
        
        # Fallback to reasoning engine
        reasoning_result = reasoning_engine.reasoning_predict(text)
        
        result = {
            "input": text,
            "intent": reasoning_result["intent"],
            "command": reasoning_result.get("command", "unknown"),
            "entities": [],
            "confidence": reasoning_result["confidence"],
            "method": "reasoning_engine_fallback",
            "explanation": reasoning_result.get("explanation", "")
        }
    
    # Format output
    print("K·∫øt qu·∫£ d·ª± ƒëo√°n:")
    print(f"  Input: {result['input']}")
    print(f"  Intent: {result['intent']}")
    print(f"  Command: {result['command']}")
    print(f"  Confidence: {result['confidence']:.3f}")
    print(f"  Method: {result['method']}")
    if 'explanation' in result and result['explanation']:
        print(f"  Explanation: {result['explanation']}")
    print("  Entities:")
    for entity in result['entities']:
        print(f"    - {entity['type']}: '{entity['text']}'")
    
    return result

def test_reasoning_engine():
    """Test reasoning engine v·ªõi c√°c m·∫´u kh√°c nhau"""
    print("=== TEST REASONING ENGINE ===")
    
    reasoning_engine = ReasoningEngine()
    
    test_samples = [
        "nh·∫Øc t√¥i l√∫c 5 gi·ªù chi·ªÅu",
        "alo cho b·ªë",
        "g·ª≠i tin nh·∫Øn cho m·∫π",
        "ƒë·∫∑t b√°o th·ª©c l√∫c 7 gi·ªù s√°ng",
        "th·ªùi ti·∫øt h√¥m nay th·∫ø n√†o",
        "t√¥i mu·ªën nghe nh·∫°c",
        "ƒë·ªçc tin t·ª©c cho t√¥i",
        "m·ªü ·ª©ng d·ª•ng Zalo",
        "t√¨m ki·∫øm th√¥ng tin v·ªÅ b·ªánh ti·ªÉu ƒë∆∞·ªùng",
        "g·ªçi video cho con g√°i",
        "t√¥i mu·ªën xem phim h√†i",
        "ki·ªÉm tra s·ª©c kh·ªèe c·ªßa t√¥i",
        "xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?"
    ]
    
    for i, text in enumerate(test_samples, 1):
        print(f"\n--- Test {i}: '{text}' ---")
        try:
            result = reasoning_engine.reasoning_predict(text)
            print(f"Intent: {result['intent']}")
            print(f"Confidence: {result['confidence']:.3f}")
            if 'explanation' in result and result['explanation']:
                print(f"Explanation: {result['explanation']}")
        except Exception as e:
            print(f"‚ùå L·ªói: {str(e)}")
    
    print("\n=== HO√ÄN TH√ÄNH TEST REASONING ENGINE ===")

def main():
    parser = argparse.ArgumentParser(description="PhoBERT_SAM - Intent Recognition, Entity Extraction v√† Command Processing")
    parser.add_argument("--mode", choices=["train", "test", "predict", "reasoning", "both"], default="both",
                       help="Ch·∫ø ƒë·ªô ch·∫°y: train, test, predict, reasoning, ho·∫∑c both")
    parser.add_argument("--text", type=str, default="",
                       help="Text ƒë·ªÉ d·ª± ƒëo√°n (ch·ªâ d√πng v·ªõi mode predict)")
    
    args = parser.parse_args()
    
    if args.mode in ["train", "both"]:
        train_models()
    
    if args.mode in ["test", "both"]:
        test_models()
    
    if args.mode == "reasoning":
        test_reasoning_engine()
    
    if args.mode == "predict":
        if args.text:
            predict_command(args.text)
        else:
            print("Vui l√≤ng cung c·∫•p text ƒë·ªÉ d·ª± ƒëo√°n v·ªõi --text")

if __name__ == "__main__":
    main() 